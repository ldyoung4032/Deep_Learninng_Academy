{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중단점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "df = pd.read_csv(\"wine.csv\", header = None)\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "dataset = df.values\n",
    "X = dataset[:,0:12]\n",
    "Y = dataset[:,12]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30,input_dim = 12, activation = 'relu'))\n",
    "model.add(Dense(12, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38286, saving model to ./model\\01-0.3829.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38286 to 0.29474, saving model to ./model\\02-0.2947.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.29474 to 0.28058, saving model to ./model\\03-0.2806.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.28058 to 0.26377, saving model to ./model\\04-0.2638.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26377 to 0.25514, saving model to ./model\\05-0.2551.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.25514 to 0.24117, saving model to ./model\\06-0.2412.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24117 to 0.23533, saving model to ./model\\07-0.2353.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23533 to 0.22850, saving model to ./model\\08-0.2285.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.22850 to 0.22045, saving model to ./model\\09-0.2204.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22045 to 0.21533, saving model to ./model\\10-0.2153.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.21533 to 0.20428, saving model to ./model\\11-0.2043.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.20428 to 0.19698, saving model to ./model\\12-0.1970.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.19698 to 0.19285, saving model to ./model\\13-0.1928.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.19285 to 0.18909, saving model to ./model\\14-0.1891.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.18909 to 0.18797, saving model to ./model\\15-0.1880.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.18797 to 0.18489, saving model to ./model\\16-0.1849.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.18489 to 0.18402, saving model to ./model\\17-0.1840.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.18402 to 0.18219, saving model to ./model\\18-0.1822.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.18219 to 0.18148, saving model to ./model\\19-0.1815.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.18148 to 0.18109, saving model to ./model\\20-0.1811.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.18109 to 0.17826, saving model to ./model\\21-0.1783.hdf5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.17826\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.17826 to 0.17790, saving model to ./model\\23-0.1779.hdf5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.17790 to 0.17714, saving model to ./model\\24-0.1771.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.17714 to 0.17521, saving model to ./model\\25-0.1752.hdf5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.17521\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.17521 to 0.17352, saving model to ./model\\27-0.1735.hdf5\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.17352 to 0.17283, saving model to ./model\\28-0.1728.hdf5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.17283\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.17283 to 0.17141, saving model to ./model\\30-0.1714.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.17141 to 0.16842, saving model to ./model\\31-0.1684.hdf5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.16842\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.16842 to 0.16681, saving model to ./model\\33-0.1668.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.16681 to 0.16667, saving model to ./model\\34-0.1667.hdf5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.16667\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.16667 to 0.16351, saving model to ./model\\36-0.1635.hdf5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.16351\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.16351 to 0.16124, saving model to ./model\\38-0.1612.hdf5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.16124\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.16124 to 0.15965, saving model to ./model\\40-0.1596.hdf5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15965\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.15965 to 0.15696, saving model to ./model\\42-0.1570.hdf5\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.15696 to 0.15660, saving model to ./model\\43-0.1566.hdf5\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.15660 to 0.15404, saving model to ./model\\44-0.1540.hdf5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.15404 to 0.15310, saving model to ./model\\45-0.1531.hdf5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.15310\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.15310 to 0.15247, saving model to ./model\\47-0.1525.hdf5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15247\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.15247 to 0.14654, saving model to ./model\\49-0.1465.hdf5\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.14654 to 0.14429, saving model to ./model\\50-0.1443.hdf5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.14429\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.14429 to 0.14372, saving model to ./model\\52-0.1437.hdf5\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.14372 to 0.14221, saving model to ./model\\53-0.1422.hdf5\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.14221 to 0.13982, saving model to ./model\\54-0.1398.hdf5\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.13982 to 0.13864, saving model to ./model\\55-0.1386.hdf5\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.13864 to 0.13687, saving model to ./model\\56-0.1369.hdf5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.13687\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.13687 to 0.13492, saving model to ./model\\58-0.1349.hdf5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.13492\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.13492\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.13492\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.13492 to 0.13441, saving model to ./model\\62-0.1344.hdf5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.13441\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.13441\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.13441 to 0.13411, saving model to ./model\\65-0.1341.hdf5\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.13411 to 0.13042, saving model to ./model\\66-0.1304.hdf5\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.13042 to 0.12892, saving model to ./model\\67-0.1289.hdf5\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.12892 to 0.12770, saving model to ./model\\68-0.1277.hdf5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.12770 to 0.12637, saving model to ./model\\69-0.1264.hdf5\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.12637 to 0.12136, saving model to ./model\\70-0.1214.hdf5\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.12136 to 0.11994, saving model to ./model\\71-0.1199.hdf5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.11994\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.11994\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.11994\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.11994 to 0.11607, saving model to ./model\\75-0.1161.hdf5\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.11607 to 0.11563, saving model to ./model\\76-0.1156.hdf5\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.11563 to 0.11445, saving model to ./model\\77-0.1144.hdf5\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.11445 to 0.11356, saving model to ./model\\78-0.1136.hdf5\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.11356 to 0.11294, saving model to ./model\\79-0.1129.hdf5\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.11294 to 0.11189, saving model to ./model\\80-0.1119.hdf5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.11189\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.11189 to 0.11098, saving model to ./model\\82-0.1110.hdf5\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.11098 to 0.11089, saving model to ./model\\83-0.1109.hdf5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.11089 to 0.10901, saving model to ./model\\84-0.1090.hdf5\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.10901 to 0.10780, saving model to ./model\\85-0.1078.hdf5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.10780 to 0.10715, saving model to ./model\\86-0.1072.hdf5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.10715 to 0.10642, saving model to ./model\\87-0.1064.hdf5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.10642 to 0.10533, saving model to ./model\\88-0.1053.hdf5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.10533 to 0.10490, saving model to ./model\\89-0.1049.hdf5\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.10490 to 0.10487, saving model to ./model\\90-0.1049.hdf5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.10487 to 0.10312, saving model to ./model\\91-0.1031.hdf5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.10312 to 0.10304, saving model to ./model\\92-0.1030.hdf5\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.10304 to 0.10249, saving model to ./model\\93-0.1025.hdf5\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.10249 to 0.10064, saving model to ./model\\94-0.1006.hdf5\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.10064\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.10064\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.10064 to 0.10004, saving model to ./model\\97-0.1000.hdf5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.10004 to 0.09942, saving model to ./model\\98-0.0994.hdf5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.09942\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.09942\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.09942 to 0.09638, saving model to ./model\\101-0.0964.hdf5\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.09638 to 0.09566, saving model to ./model\\102-0.0957.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00103: val_loss did not improve from 0.09566\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.09566\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.09566\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.09566\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.09566\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.09566 to 0.09073, saving model to ./model\\108-0.0907.hdf5\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.09073 to 0.08976, saving model to ./model\\109-0.0898.hdf5\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.08976 to 0.08906, saving model to ./model\\110-0.0891.hdf5\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.08906 to 0.08807, saving model to ./model\\111-0.0881.hdf5\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.08807\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.08807\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.08807\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.08807 to 0.08615, saving model to ./model\\115-0.0862.hdf5\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.08615 to 0.08590, saving model to ./model\\116-0.0859.hdf5\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.08590 to 0.08490, saving model to ./model\\117-0.0849.hdf5\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.08490 to 0.08476, saving model to ./model\\118-0.0848.hdf5\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.08476 to 0.08382, saving model to ./model\\119-0.0838.hdf5\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.08382\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.08382\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.08382 to 0.08305, saving model to ./model\\122-0.0831.hdf5\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.08305 to 0.08257, saving model to ./model\\123-0.0826.hdf5\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.08257\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.08257 to 0.08217, saving model to ./model\\125-0.0822.hdf5\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.08217\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.08217\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.08217 to 0.08173, saving model to ./model\\128-0.0817.hdf5\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.08173\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.08173\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.08173 to 0.07971, saving model to ./model\\131-0.0797.hdf5\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.07971\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.07971 to 0.07934, saving model to ./model\\133-0.0793.hdf5\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.07934\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.07934\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.07934 to 0.07820, saving model to ./model\\136-0.0782.hdf5\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.07820 to 0.07798, saving model to ./model\\137-0.0780.hdf5\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.07798\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.07798\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.07798 to 0.07753, saving model to ./model\\140-0.0775.hdf5\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.07753\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.07753\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.07753 to 0.07543, saving model to ./model\\143-0.0754.hdf5\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.07543\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.07543 to 0.07542, saving model to ./model\\145-0.0754.hdf5\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.07542 to 0.07428, saving model to ./model\\146-0.0743.hdf5\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.07428 to 0.07400, saving model to ./model\\147-0.0740.hdf5\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.07400 to 0.07391, saving model to ./model\\148-0.0739.hdf5\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.07391 to 0.07372, saving model to ./model\\149-0.0737.hdf5\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.07372\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.07372\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.07372 to 0.07363, saving model to ./model\\152-0.0736.hdf5\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.07363 to 0.07359, saving model to ./model\\153-0.0736.hdf5\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.07359\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.07359\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.07359\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.07359\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.07359 to 0.07151, saving model to ./model\\158-0.0715.hdf5\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.07151 to 0.07144, saving model to ./model\\159-0.0714.hdf5\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.07144 to 0.07084, saving model to ./model\\160-0.0708.hdf5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.07084\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.07084 to 0.07023, saving model to ./model\\162-0.0702.hdf5\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.07023\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.07023\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.07023\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.07023 to 0.07018, saving model to ./model\\166-0.0702.hdf5\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.07018 to 0.06907, saving model to ./model\\167-0.0691.hdf5\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.06907 to 0.06844, saving model to ./model\\168-0.0684.hdf5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.06844 to 0.06824, saving model to ./model\\169-0.0682.hdf5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.06824\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.06824 to 0.06816, saving model to ./model\\171-0.0682.hdf5\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.06816\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.06816 to 0.06776, saving model to ./model\\173-0.0678.hdf5\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.06776\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.06776\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.06776\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.06776 to 0.06692, saving model to ./model\\177-0.0669.hdf5\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.06692\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.06692 to 0.06564, saving model to ./model\\179-0.0656.hdf5\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.06564 to 0.06554, saving model to ./model\\180-0.0655.hdf5\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.06554\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.06554\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.06554 to 0.06551, saving model to ./model\\183-0.0655.hdf5\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.06551\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.06551\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.06551\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.06551 to 0.06427, saving model to ./model\\187-0.0643.hdf5\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.06427 to 0.06389, saving model to ./model\\188-0.0639.hdf5\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.06389\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.06389\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.06389\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.06389 to 0.06344, saving model to ./model\\192-0.0634.hdf5\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.06344\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.06344 to 0.06305, saving model to ./model\\194-0.0630.hdf5\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.06305 to 0.06231, saving model to ./model\\195-0.0623.hdf5\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.06231\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.06231 to 0.06222, saving model to ./model\\197-0.0622.hdf5\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.06222 to 0.06219, saving model to ./model\\198-0.0622.hdf5\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.06219\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.06219 to 0.06152, saving model to ./model\\200-0.0615.hdf5\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.06152\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.06152\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.06152\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.06152\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.06152\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.06152 to 0.06125, saving model to ./model\\206-0.0613.hdf5\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.06125 to 0.06076, saving model to ./model\\207-0.0608.hdf5\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.06076\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.06076\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.06076\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.06076\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.06076 to 0.06037, saving model to ./model\\212-0.0604.hdf5\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.06037 to 0.06011, saving model to ./model\\213-0.0601.hdf5\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.06011\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.06011\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.06011 to 0.05908, saving model to ./model\\216-0.0591.hdf5\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.05908\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.05908 to 0.05840, saving model to ./model\\218-0.0584.hdf5\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.05840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00222: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.05840\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.05840 to 0.05780, saving model to ./model\\230-0.0578.hdf5\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.05780\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.05780\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.05780 to 0.05722, saving model to ./model\\233-0.0572.hdf5\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.05722 to 0.05712, saving model to ./model\\234-0.0571.hdf5\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.05712 to 0.05683, saving model to ./model\\235-0.0568.hdf5\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.05683\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.05683 to 0.05676, saving model to ./model\\237-0.0568.hdf5\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.05676 to 0.05597, saving model to ./model\\238-0.0560.hdf5\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.05597\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.05597\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.05597\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.05597\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.05597 to 0.05492, saving model to ./model\\243-0.0549.hdf5\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.05492\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.05492\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.05492\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.05492\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.05492\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.05492\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.05492\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.05492\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.05492 to 0.05384, saving model to ./model\\252-0.0538.hdf5\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.05384\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.05384\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.05384 to 0.05362, saving model to ./model\\255-0.0536.hdf5\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.05362\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.05362 to 0.05325, saving model to ./model\\257-0.0532.hdf5\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.05325\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.05325\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.05325 to 0.05268, saving model to ./model\\260-0.0527.hdf5\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.05268\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.05268\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.05268 to 0.05245, saving model to ./model\\263-0.0524.hdf5\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.05245\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.05245 to 0.05238, saving model to ./model\\265-0.0524.hdf5\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.05238\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.05238\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.05238\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.05238\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.05238\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.05238\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.05238 to 0.05206, saving model to ./model\\272-0.0521.hdf5\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.05206\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.05206\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.05206 to 0.05204, saving model to ./model\\275-0.0520.hdf5\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.05204\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.05204\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.05204\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.05204 to 0.05190, saving model to ./model\\279-0.0519.hdf5\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.05190\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.05190 to 0.05148, saving model to ./model\\293-0.0515.hdf5\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.05148\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.05148 to 0.05125, saving model to ./model\\306-0.0513.hdf5\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.05125\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.05125\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.05125 to 0.05121, saving model to ./model\\309-0.0512.hdf5\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.05121\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.05121 to 0.05105, saving model to ./model\\311-0.0511.hdf5\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.05105\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.05105\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.05105 to 0.05057, saving model to ./model\\314-0.0506.hdf5\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.05057\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.05057\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.05057\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.05057\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.05057\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.05057\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.05057\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.05057 to 0.04994, saving model to ./model\\322-0.0499.hdf5\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.04994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00367: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.04994\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.04994 to 0.04955, saving model to ./model\\372-0.0495.hdf5\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.04955\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.04955 to 0.04943, saving model to ./model\\397-0.0494.hdf5\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.04943\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.04943 to 0.04910, saving model to ./model\\399-0.0491.hdf5\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.04910\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.04910\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "modelpath = './model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True) ## save_best_only=True 성능이 좋아질때만 저장\n",
    "\n",
    "history = model.fit(X,Y,validation_split=0.33, epochs=3500, batch_size = 500, verbose=0, callbacks = [early_stopping_callback,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 435us/step - loss: 0.0517 - accuracy: 0.9869\n",
      "\n",
      " Accurcy : 0.9869\n"
     ]
    }
   ],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c='red', markersize = 3)\n",
    "plt.plot(x_len, y_acc, \"o\", c='blue', markersize = 3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Accurcy : %.4f\"%(model.evaluate(X,Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 56.93710, saving model to ./model\\01-56.9371.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 56.93710 to 35.07482, saving model to ./model\\02-35.0748.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 35.07482\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 35.07482\n",
      "\n",
      "Epoch 00005: val_loss improved from 35.07482 to 30.19500, saving model to ./model\\05-30.1950.hdf5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 30.19500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 30.19500\n",
      "\n",
      "Epoch 00008: val_loss improved from 30.19500 to 28.69428, saving model to ./model\\08-28.6943.hdf5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 28.69428\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 28.69428\n",
      "\n",
      "Epoch 00011: val_loss improved from 28.69428 to 27.30036, saving model to ./model\\11-27.3004.hdf5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 27.30036\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 27.30036\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 27.30036\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 27.30036\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 27.30036\n",
      "\n",
      "Epoch 00017: val_loss improved from 27.30036 to 26.61930, saving model to ./model\\17-26.6193.hdf5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 26.61930\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 26.61930\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 26.61930\n",
      "\n",
      "Epoch 00021: val_loss improved from 26.61930 to 26.60098, saving model to ./model\\21-26.6010.hdf5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 26.60098\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 26.60098\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.60098\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.60098 to 25.19775, saving model to ./model\\25-25.1977.hdf5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.19775\n",
      "\n",
      "Epoch 00027: val_loss improved from 25.19775 to 23.11386, saving model to ./model\\27-23.1139.hdf5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.11386\n",
      "\n",
      "Epoch 00037: val_loss improved from 23.11386 to 22.78386, saving model to ./model\\37-22.7839.hdf5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.78386\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.78386\n",
      "\n",
      "Epoch 00040: val_loss improved from 22.78386 to 21.93160, saving model to ./model\\40-21.9316.hdf5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21.93160\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21.93160\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.93160\n",
      "\n",
      "Epoch 00044: val_loss improved from 21.93160 to 20.07876, saving model to ./model\\44-20.0788.hdf5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 20.07876\n",
      "\n",
      "Epoch 00072: val_loss improved from 20.07876 to 20.03066, saving model to ./model\\72-20.0307.hdf5\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 20.03066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00153: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 20.03066\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 20.03066\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 750us/step - loss: 24.1212 - accuracy: 0.0000e+00\n",
      "\n",
      " Accurcy : 0.0000\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001999198C040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "실제가격: 17.400, 예상가격: 24.228\n",
      "실제가격: 44.800, 예상가격: 42.513\n",
      "실제가격: 29.100, 예상가격: 29.477\n",
      "실제가격: 19.800, 예상가격: 30.673\n",
      "실제가격: 22.800, 예상가격: 31.363\n",
      "실제가격: 46.000, 예상가격: 36.087\n",
      "실제가격: 9.700, 예상가격: 10.666\n",
      "실제가격: 18.500, 예상가격: 27.862\n",
      "실제가격: 31.500, 예상가격: 33.074\n",
      "실제가격: 35.400, 예상가격: 45.402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"housing.csv\", delim_whitespace = True, header = None, names = ['CRIM', 'INDUS', 'NOX', 'RM', 'LSTAT', 'B', 'PTRATIO', 'ZN', 'CHAS', 'AGE', 'RAD', 'DIS', 'TAX', 'PRICE'])\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "df = df.sample(frac = 0.5)\n",
    "\n",
    "dataset = df.values\n",
    "X = dataset[:,0:13].astype('float')\n",
    "Y = dataset[:,13]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=13, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "modelpath = './model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True) ## save_best_only=True 성능이 좋아질때만 저장\n",
    "\n",
    "history = model.fit(X_train,Y_train,validation_split=0.33, epochs=3500, batch_size = 20, verbose=0, callbacks = [early_stopping_callback,checkpoint])\n",
    "\n",
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c='red', markersize = 3)\n",
    "plt.plot(x_len, y_acc, \"o\", c='blue', markersize = 3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Accurcy : %.4f\"%(model.evaluate(X,Y)[1]))\n",
    "\n",
    "# 예측 값과 실제 값의 비교\n",
    "Y_prediction = model.predict(X_test).flatten()\n",
    "for i in range(10):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction)) ## 빨간색은 학습셋 정확도\n",
    "    ## 파란색은 테스트셋 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1   2   3    4     5      6   7  8\n",
       "0     6  148  72  35    0  33.6  0.627  50  1\n",
       "1     1   85  66  29    0  26.6  0.351  31  0\n",
       "2     8  183  64   0    0  23.3  0.672  32  1\n",
       "3     1   89  66  23   94  28.1  0.167  21  0\n",
       "4     0  137  40  35  168  43.1  2.288  33  1\n",
       "..   ..  ...  ..  ..  ...   ...    ...  .. ..\n",
       "763  10  101  76  48  180  32.9  0.171  63  0\n",
       "764   2  122  70  27    0  36.8  0.340  27  0\n",
       "765   5  121  72  23  112  26.2  0.245  30  0\n",
       "766   1  126  60   0    0  30.1  0.349  47  1\n",
       "767   1   93  70  31    0  30.4  0.315  23  0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"pima-indians-diabetes.csv\", header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.15831, saving model to ./model\\01-9.1583.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 9.15831 to 8.67320, saving model to ./model\\02-8.6732.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 8.67320 to 8.19884, saving model to ./model\\03-8.1988.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 8.19884 to 7.73880, saving model to ./model\\04-7.7388.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 7.73880 to 7.29319, saving model to ./model\\05-7.2932.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 7.29319 to 6.85520, saving model to ./model\\06-6.8552.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.85520 to 6.42175, saving model to ./model\\07-6.4218.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.42175 to 5.98443, saving model to ./model\\08-5.9844.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 5.98443 to 5.56037, saving model to ./model\\09-5.5604.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 5.56037 to 5.15332, saving model to ./model\\10-5.1533.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 5.15332 to 4.77100, saving model to ./model\\11-4.7710.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 4.77100 to 4.38311, saving model to ./model\\12-4.3831.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 4.38311 to 4.01167, saving model to ./model\\13-4.0117.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 4.01167 to 3.65567, saving model to ./model\\14-3.6557.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.65567 to 3.31835, saving model to ./model\\15-3.3184.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.31835 to 2.98267, saving model to ./model\\16-2.9827.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.98267 to 2.67778, saving model to ./model\\17-2.6778.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.67778 to 2.38324, saving model to ./model\\18-2.3832.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.38324 to 2.10337, saving model to ./model\\19-2.1034.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.10337 to 1.85815, saving model to ./model\\20-1.8581.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.85815 to 1.67374, saving model to ./model\\21-1.6737.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.67374 to 1.54054, saving model to ./model\\22-1.5405.hdf5\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.54054 to 1.42687, saving model to ./model\\23-1.4269.hdf5\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.42687 to 1.35451, saving model to ./model\\24-1.3545.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.35451 to 1.30620, saving model to ./model\\25-1.3062.hdf5\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.30620 to 1.26571, saving model to ./model\\26-1.2657.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.26571 to 1.23680, saving model to ./model\\27-1.2368.hdf5\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.23680 to 1.20802, saving model to ./model\\28-1.2080.hdf5\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.20802 to 1.18603, saving model to ./model\\29-1.1860.hdf5\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.18603 to 1.16400, saving model to ./model\\30-1.1640.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.16400 to 1.14510, saving model to ./model\\31-1.1451.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.14510 to 1.12512, saving model to ./model\\32-1.1251.hdf5\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.12512 to 1.10989, saving model to ./model\\33-1.1099.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.10989 to 1.09438, saving model to ./model\\34-1.0944.hdf5\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.09438 to 1.08066, saving model to ./model\\35-1.0807.hdf5\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.08066 to 1.07078, saving model to ./model\\36-1.0708.hdf5\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.07078 to 1.06675, saving model to ./model\\37-1.0667.hdf5\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.06675 to 1.06206, saving model to ./model\\38-1.0621.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.06206 to 1.05470, saving model to ./model\\39-1.0547.hdf5\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.05470 to 1.04564, saving model to ./model\\40-1.0456.hdf5\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.04564 to 1.03938, saving model to ./model\\41-1.0394.hdf5\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.03938 to 1.03061, saving model to ./model\\42-1.0306.hdf5\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.03061 to 1.02360, saving model to ./model\\43-1.0236.hdf5\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.02360 to 1.01582, saving model to ./model\\44-1.0158.hdf5\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.01582 to 1.00780, saving model to ./model\\45-1.0078.hdf5\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.00780 to 0.99927, saving model to ./model\\46-0.9993.hdf5\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.99927 to 0.99353, saving model to ./model\\47-0.9935.hdf5\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.99353 to 0.98947, saving model to ./model\\48-0.9895.hdf5\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.98947 to 0.98323, saving model to ./model\\49-0.9832.hdf5\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.98323 to 0.97464, saving model to ./model\\50-0.9746.hdf5\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.97464 to 0.96403, saving model to ./model\\51-0.9640.hdf5\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.96403 to 0.95704, saving model to ./model\\52-0.9570.hdf5\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.95704 to 0.94740, saving model to ./model\\53-0.9474.hdf5\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.94740 to 0.93275, saving model to ./model\\54-0.9327.hdf5\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.93275 to 0.92329, saving model to ./model\\55-0.9233.hdf5\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.92329 to 0.91350, saving model to ./model\\56-0.9135.hdf5\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.91350 to 0.90246, saving model to ./model\\57-0.9025.hdf5\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.90246 to 0.89163, saving model to ./model\\58-0.8916.hdf5\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.89163 to 0.88026, saving model to ./model\\59-0.8803.hdf5\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.88026 to 0.87014, saving model to ./model\\60-0.8701.hdf5\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.87014 to 0.86132, saving model to ./model\\61-0.8613.hdf5\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.86132 to 0.85422, saving model to ./model\\62-0.8542.hdf5\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.85422 to 0.84797, saving model to ./model\\63-0.8480.hdf5\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.84797 to 0.84112, saving model to ./model\\64-0.8411.hdf5\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.84112 to 0.83273, saving model to ./model\\65-0.8327.hdf5\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.83273 to 0.82334, saving model to ./model\\66-0.8233.hdf5\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.82334 to 0.81621, saving model to ./model\\67-0.8162.hdf5\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.81621 to 0.81092, saving model to ./model\\68-0.8109.hdf5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.81092 to 0.80803, saving model to ./model\\69-0.8080.hdf5\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.80803 to 0.80379, saving model to ./model\\70-0.8038.hdf5\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.80379 to 0.79881, saving model to ./model\\71-0.7988.hdf5\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.79881 to 0.79041, saving model to ./model\\72-0.7904.hdf5\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.79041 to 0.78387, saving model to ./model\\73-0.7839.hdf5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.78387 to 0.77996, saving model to ./model\\74-0.7800.hdf5\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.77996 to 0.77868, saving model to ./model\\75-0.7787.hdf5\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.77868 to 0.77676, saving model to ./model\\76-0.7768.hdf5\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.77676 to 0.77280, saving model to ./model\\77-0.7728.hdf5\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.77280 to 0.76870, saving model to ./model\\78-0.7687.hdf5\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.76870 to 0.76417, saving model to ./model\\79-0.7642.hdf5\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.76417 to 0.76142, saving model to ./model\\80-0.7614.hdf5\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.76142 to 0.75960, saving model to ./model\\81-0.7596.hdf5\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.75960 to 0.75717, saving model to ./model\\82-0.7572.hdf5\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.75717 to 0.75528, saving model to ./model\\83-0.7553.hdf5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.75528 to 0.75299, saving model to ./model\\84-0.7530.hdf5\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.75299 to 0.75020, saving model to ./model\\85-0.7502.hdf5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.75020 to 0.74650, saving model to ./model\\86-0.7465.hdf5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.74650 to 0.74468, saving model to ./model\\87-0.7447.hdf5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.74468 to 0.74297, saving model to ./model\\88-0.7430.hdf5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.74297 to 0.74147, saving model to ./model\\89-0.7415.hdf5\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.74147 to 0.73826, saving model to ./model\\90-0.7383.hdf5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.73826 to 0.73600, saving model to ./model\\91-0.7360.hdf5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.73600 to 0.73337, saving model to ./model\\92-0.7334.hdf5\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.73337 to 0.73141, saving model to ./model\\93-0.7314.hdf5\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.73141 to 0.72983, saving model to ./model\\94-0.7298.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00095: val_loss improved from 0.72983 to 0.72809, saving model to ./model\\95-0.7281.hdf5\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.72809 to 0.72656, saving model to ./model\\96-0.7266.hdf5\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.72656 to 0.72526, saving model to ./model\\97-0.7253.hdf5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.72526 to 0.72405, saving model to ./model\\98-0.7241.hdf5\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.72405 to 0.72288, saving model to ./model\\99-0.7229.hdf5\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.72288 to 0.72152, saving model to ./model\\100-0.7215.hdf5\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.72152 to 0.72007, saving model to ./model\\101-0.7201.hdf5\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.72007 to 0.71892, saving model to ./model\\102-0.7189.hdf5\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.71892 to 0.71799, saving model to ./model\\103-0.7180.hdf5\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.71799 to 0.71628, saving model to ./model\\104-0.7163.hdf5\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.71628 to 0.71505, saving model to ./model\\105-0.7150.hdf5\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.71505 to 0.71301, saving model to ./model\\106-0.7130.hdf5\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.71301 to 0.71188, saving model to ./model\\107-0.7119.hdf5\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.71188 to 0.71042, saving model to ./model\\108-0.7104.hdf5\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.71042 to 0.70891, saving model to ./model\\109-0.7089.hdf5\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.70891 to 0.70797, saving model to ./model\\110-0.7080.hdf5\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.70797 to 0.70683, saving model to ./model\\111-0.7068.hdf5\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.70683 to 0.70563, saving model to ./model\\112-0.7056.hdf5\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.70563 to 0.70392, saving model to ./model\\113-0.7039.hdf5\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.70392 to 0.70252, saving model to ./model\\114-0.7025.hdf5\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.70252 to 0.70176, saving model to ./model\\115-0.7018.hdf5\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.70176 to 0.70036, saving model to ./model\\116-0.7004.hdf5\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.70036 to 0.69944, saving model to ./model\\117-0.6994.hdf5\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.69944 to 0.69849, saving model to ./model\\118-0.6985.hdf5\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.69849 to 0.69761, saving model to ./model\\119-0.6976.hdf5\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.69761 to 0.69687, saving model to ./model\\120-0.6969.hdf5\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.69687\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.69687\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.69687 to 0.69547, saving model to ./model\\123-0.6955.hdf5\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.69547 to 0.69299, saving model to ./model\\124-0.6930.hdf5\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.69299 to 0.69182, saving model to ./model\\125-0.6918.hdf5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.69182 to 0.69129, saving model to ./model\\126-0.6913.hdf5\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.69129 to 0.69096, saving model to ./model\\127-0.6910.hdf5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.69096 to 0.69088, saving model to ./model\\128-0.6909.hdf5\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.69088 to 0.68948, saving model to ./model\\129-0.6895.hdf5\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.68948 to 0.68869, saving model to ./model\\130-0.6887.hdf5\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.68869 to 0.68726, saving model to ./model\\131-0.6873.hdf5\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.68726 to 0.68711, saving model to ./model\\132-0.6871.hdf5\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.68711 to 0.68608, saving model to ./model\\133-0.6861.hdf5\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.68608 to 0.68537, saving model to ./model\\134-0.6854.hdf5\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.68537 to 0.68490, saving model to ./model\\135-0.6849.hdf5\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.68490 to 0.68441, saving model to ./model\\136-0.6844.hdf5\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.68441 to 0.68401, saving model to ./model\\137-0.6840.hdf5\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.68401 to 0.68391, saving model to ./model\\138-0.6839.hdf5\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.68391 to 0.68301, saving model to ./model\\139-0.6830.hdf5\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.68301\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.68301 to 0.68191, saving model to ./model\\141-0.6819.hdf5\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.68191 to 0.67962, saving model to ./model\\142-0.6796.hdf5\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.67962 to 0.67873, saving model to ./model\\143-0.6787.hdf5\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.67873\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.67873\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.67873\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.67873 to 0.67850, saving model to ./model\\147-0.6785.hdf5\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.67850 to 0.67630, saving model to ./model\\148-0.6763.hdf5\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.67630 to 0.67439, saving model to ./model\\149-0.6744.hdf5\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.67439 to 0.67339, saving model to ./model\\150-0.6734.hdf5\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.67339\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.67339\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.67339\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.67339\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.67339 to 0.67191, saving model to ./model\\155-0.6719.hdf5\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.67191 to 0.67032, saving model to ./model\\156-0.6703.hdf5\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.67032\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.67032\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.67032\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.67032\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.67032\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.67032\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.67032 to 0.66983, saving model to ./model\\163-0.6698.hdf5\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.66983\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.66983 to 0.66904, saving model to ./model\\165-0.6690.hdf5\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.66904 to 0.66792, saving model to ./model\\166-0.6679.hdf5\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.66792\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.66792\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.66792\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.66792\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.66792\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.66792\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.66792\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.66792 to 0.66784, saving model to ./model\\174-0.6678.hdf5\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.66784 to 0.66646, saving model to ./model\\175-0.6665.hdf5\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.66646 to 0.66584, saving model to ./model\\176-0.6658.hdf5\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.66584\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.66584\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.66584 to 0.66529, saving model to ./model\\179-0.6653.hdf5\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.66529\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.66529 to 0.66446, saving model to ./model\\181-0.6645.hdf5\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.66446 to 0.66308, saving model to ./model\\182-0.6631.hdf5\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.66308\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.66308 to 0.66229, saving model to ./model\\192-0.6623.hdf5\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.66229\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.66229\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.66229\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.66229\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.66229\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.66229\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.66229\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.66229 to 0.66047, saving model to ./model\\200-0.6605.hdf5\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.66047 to 0.65969, saving model to ./model\\201-0.6597.hdf5\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.65969\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.65969\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.65969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00205: val_loss did not improve from 0.65969\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.65969\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.65969\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.65969 to 0.65777, saving model to ./model\\208-0.6578.hdf5\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.65777\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.65777\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.65777\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.65777\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.65777 to 0.65760, saving model to ./model\\213-0.6576.hdf5\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.65760\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.65760\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.65760\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.65760\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.65760\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.65760\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.65760 to 0.65694, saving model to ./model\\220-0.6569.hdf5\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.65694 to 0.65515, saving model to ./model\\221-0.6551.hdf5\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.65515\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.65515\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.65515\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.65515\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.65515 to 0.65476, saving model to ./model\\226-0.6548.hdf5\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.65476\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.65476\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.65476\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.65476 to 0.65463, saving model to ./model\\230-0.6546.hdf5\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.65463\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.65463\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.65463\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.65463\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.65463 to 0.65439, saving model to ./model\\235-0.6544.hdf5\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.65439 to 0.65369, saving model to ./model\\236-0.6537.hdf5\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.65369\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.65369\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.65369 to 0.65194, saving model to ./model\\239-0.6519.hdf5\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.65194 to 0.65108, saving model to ./model\\240-0.6511.hdf5\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.65108\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.65108\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.65108\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.65108\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.65108 to 0.65088, saving model to ./model\\245-0.6509.hdf5\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.65088\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.65088 to 0.64932, saving model to ./model\\247-0.6493.hdf5\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.64932\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.64932\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.64932\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.64932\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.64932 to 0.64918, saving model to ./model\\252-0.6492.hdf5\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.64918 to 0.64791, saving model to ./model\\253-0.6479.hdf5\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.64791\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.64791\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.64791\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.64791\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.64791\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.64791 to 0.64682, saving model to ./model\\259-0.6468.hdf5\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.64682 to 0.64564, saving model to ./model\\260-0.6456.hdf5\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.64564\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.64564\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.64564\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.64564 to 0.64466, saving model to ./model\\264-0.6447.hdf5\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.64466 to 0.64242, saving model to ./model\\265-0.6424.hdf5\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.64242\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.64242\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.64242\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.64242\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.64242\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.64242 to 0.64214, saving model to ./model\\271-0.6421.hdf5\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.64214 to 0.64050, saving model to ./model\\272-0.6405.hdf5\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.64050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00350: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.64050\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.64050\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 673us/step - loss: 0.5659 - accuracy: 0.6953\n",
      "\n",
      " Accurcy : 0.6953\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001998E64BDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "실제가격: 0.000, 예상가격: 0.353\n",
      "실제가격: 0.000, 예상가격: 0.109\n",
      "실제가격: 0.000, 예상가격: 0.444\n",
      "실제가격: 1.000, 예상가격: 0.847\n",
      "실제가격: 0.000, 예상가격: 0.009\n",
      "실제가격: 0.000, 예상가격: 0.069\n",
      "실제가격: 1.000, 예상가격: 0.120\n",
      "실제가격: 0.000, 예상가격: 0.444\n",
      "실제가격: 0.000, 예상가격: 0.444\n",
      "실제가격: 0.000, 예상가격: 0.467\n"
     ]
    }
   ],
   "source": [
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "df = df.sample(frac = 0.5)\n",
    "\n",
    "dataset = df.values\n",
    "X = dataset[:,0:8].astype('float')\n",
    "Y = dataset[:,8]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "modelpath = './model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True) ## save_best_only=True 성능이 좋아질때만 저장\n",
    "\n",
    "history = model.fit(X_train,Y_train,validation_split=0.33, epochs=1500, batch_size = 64, verbose=0, callbacks = [early_stopping_callback,checkpoint])\n",
    "\n",
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c='red', markersize = 3)\n",
    "plt.plot(x_len, y_acc, \"o\", c='blue', markersize = 3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Accurcy : %.4f\"%(model.evaluate(X,Y)[1]))\n",
    "\n",
    "# 예측 값과 실제 값의 비교\n",
    "Y_prediction = model.predict(X_test).flatten()\n",
    "for i in range(10):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1   2   3    4     5      6   7  8\n",
       "0     6  148  72  35    0  33.6  0.627  50  1\n",
       "1     1   85  66  29    0  26.6  0.351  31  0\n",
       "2     8  183  64   0    0  23.3  0.672  32  1\n",
       "3     1   89  66  23   94  28.1  0.167  21  0\n",
       "4     0  137  40  35  168  43.1  2.288  33  1\n",
       "..   ..  ...  ..  ..  ...   ...    ...  .. ..\n",
       "763  10  101  76  48  180  32.9  0.171  63  0\n",
       "764   2  122  70  27    0  36.8  0.340  27  0\n",
       "765   5  121  72  23  112  26.2  0.245  30  0\n",
       "766   1  126  60   0    0  30.1  0.349  47  1\n",
       "767   1   93  70  31    0  30.4  0.315  23  0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"pima-indians-diabetes.csv\", header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 10.8398 - accuracy: 0.6343\n",
      "Epoch 2/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 10.0062 - accuracy: 0.6343\n",
      "Epoch 3/1500\n",
      "5/5 [==============================] - 0s 998us/step - loss: 9.2451 - accuracy: 0.6343\n",
      "Epoch 4/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 8.4604 - accuracy: 0.6381\n",
      "Epoch 5/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 7.7860 - accuracy: 0.6418\n",
      "Epoch 6/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 6.9885 - accuracy: 0.6418\n",
      "Epoch 7/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 6.3081 - accuracy: 0.6343\n",
      "Epoch 8/1500\n",
      "5/5 [==============================] - 0s 802us/step - loss: 5.6465 - accuracy: 0.6381\n",
      "Epoch 9/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 4.9760 - accuracy: 0.6381\n",
      "Epoch 10/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 4.3598 - accuracy: 0.6343\n",
      "Epoch 11/1500\n",
      "5/5 [==============================] - 0s 808us/step - loss: 3.7641 - accuracy: 0.6306\n",
      "Epoch 12/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 3.2628 - accuracy: 0.6455\n",
      "Epoch 13/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 2.8394 - accuracy: 0.6418\n",
      "Epoch 14/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 2.5055 - accuracy: 0.6604\n",
      "Epoch 15/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 2.2315 - accuracy: 0.6530\n",
      "Epoch 16/1500\n",
      "5/5 [==============================] - 0s 578us/step - loss: 2.0401 - accuracy: 0.6418\n",
      "Epoch 17/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.8489 - accuracy: 0.6343\n",
      "Epoch 18/1500\n",
      "5/5 [==============================] - 0s 628us/step - loss: 1.7168 - accuracy: 0.6418\n",
      "Epoch 19/1500\n",
      "5/5 [==============================] - 0s 795us/step - loss: 1.5987 - accuracy: 0.6343\n",
      "Epoch 20/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 1.4926 - accuracy: 0.6194\n",
      "Epoch 21/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 1.4012 - accuracy: 0.6306\n",
      "Epoch 22/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 1.3226 - accuracy: 0.6306\n",
      "Epoch 23/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 1.2573 - accuracy: 0.6231\n",
      "Epoch 24/1500\n",
      "5/5 [==============================] - 0s 809us/step - loss: 1.1931 - accuracy: 0.5933\n",
      "Epoch 25/1500\n",
      "5/5 [==============================] - 0s 410us/step - loss: 1.1507 - accuracy: 0.5858\n",
      "Epoch 26/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 1.1060 - accuracy: 0.5709\n",
      "Epoch 27/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 1.0744 - accuracy: 0.5784\n",
      "Epoch 28/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 1.0364 - accuracy: 0.5784\n",
      "Epoch 29/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.0147 - accuracy: 0.5784\n",
      "Epoch 30/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.9893 - accuracy: 0.5970\n",
      "Epoch 31/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.9642 - accuracy: 0.6007\n",
      "Epoch 32/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.9435 - accuracy: 0.6007\n",
      "Epoch 33/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.9190 - accuracy: 0.6007\n",
      "Epoch 34/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.8963 - accuracy: 0.6119\n",
      "Epoch 35/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.8773 - accuracy: 0.6082\n",
      "Epoch 36/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.8555 - accuracy: 0.6045\n",
      "Epoch 37/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.8376 - accuracy: 0.6007\n",
      "Epoch 38/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.8174 - accuracy: 0.6119\n",
      "Epoch 39/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.8057 - accuracy: 0.6194\n",
      "Epoch 40/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.8007 - accuracy: 0.6157\n",
      "Epoch 41/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.7814 - accuracy: 0.6194\n",
      "Epoch 42/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.7727 - accuracy: 0.6306\n",
      "Epoch 43/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.7851 - accuracy: 0.6082\n",
      "Epoch 44/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.7667 - accuracy: 0.6194\n",
      "Epoch 45/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.7570 - accuracy: 0.6231\n",
      "Epoch 46/1500\n",
      "5/5 [==============================] - 0s 998us/step - loss: 0.7433 - accuracy: 0.6231\n",
      "Epoch 47/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.7300 - accuracy: 0.6157\n",
      "Epoch 48/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.7198 - accuracy: 0.6194\n",
      "Epoch 49/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.7131 - accuracy: 0.6194\n",
      "Epoch 50/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.7093 - accuracy: 0.6269\n",
      "Epoch 51/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.7060 - accuracy: 0.6119\n",
      "Epoch 52/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.7013 - accuracy: 0.6119\n",
      "Epoch 53/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.6958 - accuracy: 0.6269\n",
      "Epoch 54/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.6908 - accuracy: 0.6269\n",
      "Epoch 55/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.6902 - accuracy: 0.6343\n",
      "Epoch 56/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6801 - accuracy: 0.6343\n",
      "Epoch 57/1500\n",
      "5/5 [==============================] - 0s 793us/step - loss: 0.6788 - accuracy: 0.6306\n",
      "Epoch 58/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.6758 - accuracy: 0.6157\n",
      "Epoch 59/1500\n",
      "5/5 [==============================] - 0s 629us/step - loss: 0.6691 - accuracy: 0.6231\n",
      "Epoch 60/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.6638 - accuracy: 0.6194\n",
      "Epoch 61/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6645 - accuracy: 0.6119\n",
      "Epoch 62/1500\n",
      "5/5 [==============================] - 0s 596us/step - loss: 0.6633 - accuracy: 0.6194\n",
      "Epoch 63/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.6591 - accuracy: 0.6343\n",
      "Epoch 64/1500\n",
      "5/5 [==============================] - 0s 809us/step - loss: 0.6512 - accuracy: 0.6381\n",
      "Epoch 65/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.6484 - accuracy: 0.6604\n",
      "Epoch 66/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6440 - accuracy: 0.6567\n",
      "Epoch 67/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6438 - accuracy: 0.6381\n",
      "Epoch 68/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6407 - accuracy: 0.6343\n",
      "Epoch 69/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.6347 - accuracy: 0.6455\n",
      "Epoch 70/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6295 - accuracy: 0.6493\n",
      "Epoch 71/1500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 0.6292 - accuracy: 0.6455\n",
      "Epoch 72/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.6251 - accuracy: 0.6493\n",
      "Epoch 73/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.6223 - accuracy: 0.6343\n",
      "Epoch 74/1500\n",
      "5/5 [==============================] - 0s 616us/step - loss: 0.6201 - accuracy: 0.6381\n",
      "Epoch 75/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6189 - accuracy: 0.6343\n",
      "Epoch 76/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.6170 - accuracy: 0.6343\n",
      "Epoch 77/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.6146 - accuracy: 0.6455\n",
      "Epoch 78/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.6114 - accuracy: 0.6418\n",
      "Epoch 79/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.6083 - accuracy: 0.6530\n",
      "Epoch 80/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6062 - accuracy: 0.6530\n",
      "Epoch 81/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6037 - accuracy: 0.6493\n",
      "Epoch 82/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.6028 - accuracy: 0.6567\n",
      "Epoch 83/1500\n",
      "5/5 [==============================] - 0s 810us/step - loss: 0.5989 - accuracy: 0.6530\n",
      "Epoch 84/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5975 - accuracy: 0.6530\n",
      "Epoch 85/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.5954 - accuracy: 0.6493\n",
      "Epoch 86/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5940 - accuracy: 0.6530\n",
      "Epoch 87/1500\n",
      "5/5 [==============================] - 0s 657us/step - loss: 0.5924 - accuracy: 0.6418\n",
      "Epoch 88/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.5922 - accuracy: 0.6604\n",
      "Epoch 89/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5882 - accuracy: 0.6791\n",
      "Epoch 90/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5884 - accuracy: 0.6716\n",
      "Epoch 91/1500\n",
      "5/5 [==============================] - 0s 651us/step - loss: 0.5864 - accuracy: 0.6679\n",
      "Epoch 92/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.5861 - accuracy: 0.6716\n",
      "Epoch 93/1500\n",
      "5/5 [==============================] - 0s 796us/step - loss: 0.5806 - accuracy: 0.6828\n",
      "Epoch 94/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5815 - accuracy: 0.6679\n",
      "Epoch 95/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5891 - accuracy: 0.6716\n",
      "Epoch 96/1500\n",
      "5/5 [==============================] - 0s 799us/step - loss: 0.5837 - accuracy: 0.6642\n",
      "Epoch 97/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.5822 - accuracy: 0.6679\n",
      "Epoch 98/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5826 - accuracy: 0.6754\n",
      "Epoch 99/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5775 - accuracy: 0.6940\n",
      "Epoch 100/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5801 - accuracy: 0.6978\n",
      "Epoch 101/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5826 - accuracy: 0.6903\n",
      "Epoch 102/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5763 - accuracy: 0.7015\n",
      "Epoch 103/1500\n",
      "5/5 [==============================] - 0s 659us/step - loss: 0.5746 - accuracy: 0.6903\n",
      "Epoch 104/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5738 - accuracy: 0.7052\n",
      "Epoch 105/1500\n",
      "5/5 [==============================] - 0s 604us/step - loss: 0.5727 - accuracy: 0.7090\n",
      "Epoch 106/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5727 - accuracy: 0.6978\n",
      "Epoch 107/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5732 - accuracy: 0.7052\n",
      "Epoch 108/1500\n",
      "5/5 [==============================] - 0s 582us/step - loss: 0.5733 - accuracy: 0.7052\n",
      "Epoch 109/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.5714 - accuracy: 0.7015\n",
      "Epoch 110/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5699 - accuracy: 0.7127\n",
      "Epoch 111/1500\n",
      "5/5 [==============================] - 0s 583us/step - loss: 0.5775 - accuracy: 0.6940\n",
      "Epoch 112/1500\n",
      "5/5 [==============================] - 0s 404us/step - loss: 0.5683 - accuracy: 0.6978\n",
      "Epoch 113/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5761 - accuracy: 0.6866\n",
      "Epoch 114/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5705 - accuracy: 0.6903\n",
      "Epoch 115/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5633 - accuracy: 0.7127\n",
      "Epoch 116/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.5806 - accuracy: 0.7015\n",
      "Epoch 117/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5633 - accuracy: 0.7313\n",
      "Epoch 118/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.5902 - accuracy: 0.6828\n",
      "Epoch 119/1500\n",
      "5/5 [==============================] - 0s 388us/step - loss: 0.5928 - accuracy: 0.6791\n",
      "Epoch 120/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5698 - accuracy: 0.6903\n",
      "Epoch 121/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5656 - accuracy: 0.7090\n",
      "Epoch 122/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.5608 - accuracy: 0.7052\n",
      "Epoch 123/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.5701 - accuracy: 0.6828\n",
      "Epoch 124/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.5628 - accuracy: 0.6940\n",
      "Epoch 125/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.5705 - accuracy: 0.7015\n",
      "Epoch 126/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5674 - accuracy: 0.7015\n",
      "Epoch 127/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5606 - accuracy: 0.7052\n",
      "Epoch 128/1500\n",
      "5/5 [==============================] - 0s 614us/step - loss: 0.5612 - accuracy: 0.7015\n",
      "Epoch 129/1500\n",
      "5/5 [==============================] - 0s 550us/step - loss: 0.5590 - accuracy: 0.7164\n",
      "Epoch 130/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.5608 - accuracy: 0.7052\n",
      "Epoch 131/1500\n",
      "5/5 [==============================] - 0s 810us/step - loss: 0.5577 - accuracy: 0.7015\n",
      "Epoch 132/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5703 - accuracy: 0.6903\n",
      "Epoch 133/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.5578 - accuracy: 0.7052\n",
      "Epoch 134/1500\n",
      "5/5 [==============================] - 0s 810us/step - loss: 0.5636 - accuracy: 0.7090\n",
      "Epoch 135/1500\n",
      "5/5 [==============================] - 0s 675us/step - loss: 0.5567 - accuracy: 0.7052\n",
      "Epoch 136/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5579 - accuracy: 0.6940\n",
      "Epoch 137/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5560 - accuracy: 0.7127\n",
      "Epoch 138/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5544 - accuracy: 0.7127\n",
      "Epoch 139/1500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 0.5528 - accuracy: 0.6978\n",
      "Epoch 140/1500\n",
      "5/5 [==============================] - 0s 616us/step - loss: 0.5527 - accuracy: 0.7127\n",
      "Epoch 141/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5616 - accuracy: 0.7090\n",
      "Epoch 142/1500\n",
      "5/5 [==============================] - 0s 389us/step - loss: 0.5510 - accuracy: 0.7052\n",
      "Epoch 143/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5510 - accuracy: 0.7164\n",
      "Epoch 144/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5545 - accuracy: 0.7015\n",
      "Epoch 145/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5517 - accuracy: 0.7052\n",
      "Epoch 146/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.5487 - accuracy: 0.7127\n",
      "Epoch 147/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5448 - accuracy: 0.7090\n",
      "Epoch 148/1500\n",
      "5/5 [==============================] - 0s 615us/step - loss: 0.5540 - accuracy: 0.6903\n",
      "Epoch 149/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5524 - accuracy: 0.6978\n",
      "Epoch 150/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.5529 - accuracy: 0.7052\n",
      "Epoch 151/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5492 - accuracy: 0.7164\n",
      "Epoch 152/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.5494 - accuracy: 0.6978\n",
      "Epoch 153/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5476 - accuracy: 0.7090\n",
      "Epoch 154/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5451 - accuracy: 0.7052\n",
      "Epoch 155/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5456 - accuracy: 0.7090\n",
      "Epoch 156/1500\n",
      "5/5 [==============================] - 0s 528us/step - loss: 0.5440 - accuracy: 0.7090\n",
      "Epoch 157/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5425 - accuracy: 0.7127\n",
      "Epoch 158/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5424 - accuracy: 0.7090\n",
      "Epoch 159/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5423 - accuracy: 0.7090\n",
      "Epoch 160/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5415 - accuracy: 0.7164\n",
      "Epoch 161/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.5422 - accuracy: 0.7127\n",
      "Epoch 162/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 599us/step - loss: 0.5421 - accuracy: 0.7090\n",
      "Epoch 163/1500\n",
      "5/5 [==============================] - 0s 581us/step - loss: 0.5454 - accuracy: 0.7127\n",
      "Epoch 164/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5441 - accuracy: 0.6940\n",
      "Epoch 165/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5406 - accuracy: 0.7052\n",
      "Epoch 166/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.5407 - accuracy: 0.7164\n",
      "Epoch 167/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5400 - accuracy: 0.7090\n",
      "Epoch 168/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5396 - accuracy: 0.7127\n",
      "Epoch 169/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5389 - accuracy: 0.7164\n",
      "Epoch 170/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5376 - accuracy: 0.7127\n",
      "Epoch 171/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5393 - accuracy: 0.7127\n",
      "Epoch 172/1500\n",
      "5/5 [==============================] - 0s 763us/step - loss: 0.5391 - accuracy: 0.7127\n",
      "Epoch 173/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.5509 - accuracy: 0.7015\n",
      "Epoch 174/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5479 - accuracy: 0.7127\n",
      "Epoch 175/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.5403 - accuracy: 0.7201\n",
      "Epoch 176/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5422 - accuracy: 0.6978\n",
      "Epoch 177/1500\n",
      "5/5 [==============================] - 0s 613us/step - loss: 0.5370 - accuracy: 0.7164\n",
      "Epoch 178/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5363 - accuracy: 0.7127\n",
      "Epoch 179/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5351 - accuracy: 0.7201\n",
      "Epoch 180/1500\n",
      "5/5 [==============================] - 0s 783us/step - loss: 0.5362 - accuracy: 0.7090\n",
      "Epoch 181/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.5368 - accuracy: 0.7052\n",
      "Epoch 182/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5376 - accuracy: 0.7052\n",
      "Epoch 183/1500\n",
      "5/5 [==============================] - 0s 429us/step - loss: 0.5380 - accuracy: 0.7052\n",
      "Epoch 184/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5375 - accuracy: 0.7127\n",
      "Epoch 185/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5343 - accuracy: 0.7127\n",
      "Epoch 186/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.5342 - accuracy: 0.7164\n",
      "Epoch 187/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5370 - accuracy: 0.7052\n",
      "Epoch 188/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5335 - accuracy: 0.7090\n",
      "Epoch 189/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5346 - accuracy: 0.7090\n",
      "Epoch 190/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5322 - accuracy: 0.7164\n",
      "Epoch 191/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5333 - accuracy: 0.7127\n",
      "Epoch 192/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.5338 - accuracy: 0.7164\n",
      "Epoch 193/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5335 - accuracy: 0.7201\n",
      "Epoch 194/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5383 - accuracy: 0.7090\n",
      "Epoch 195/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.5321 - accuracy: 0.7090\n",
      "Epoch 196/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.5310 - accuracy: 0.7052\n",
      "Epoch 197/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5317 - accuracy: 0.7052\n",
      "Epoch 198/1500\n",
      "5/5 [==============================] - 0s 718us/step - loss: 0.5303 - accuracy: 0.7090\n",
      "Epoch 199/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5296 - accuracy: 0.7127\n",
      "Epoch 200/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.5387 - accuracy: 0.7015\n",
      "Epoch 201/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.5299 - accuracy: 0.7164\n",
      "Epoch 202/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.5365 - accuracy: 0.7127\n",
      "Epoch 203/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5339 - accuracy: 0.7164\n",
      "Epoch 204/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5282 - accuracy: 0.7090\n",
      "Epoch 205/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.5315 - accuracy: 0.7052\n",
      "Epoch 206/1500\n",
      "5/5 [==============================] - 0s 404us/step - loss: 0.5258 - accuracy: 0.7090\n",
      "Epoch 207/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5228 - accuracy: 0.7164\n",
      "Epoch 208/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5229 - accuracy: 0.7090\n",
      "Epoch 209/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5209 - accuracy: 0.7090\n",
      "Epoch 210/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.5218 - accuracy: 0.7127\n",
      "Epoch 211/1500\n",
      "5/5 [==============================] - 0s 685us/step - loss: 0.5246 - accuracy: 0.7052\n",
      "Epoch 212/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5244 - accuracy: 0.7201\n",
      "Epoch 213/1500\n",
      "5/5 [==============================] - 0s 582us/step - loss: 0.5201 - accuracy: 0.7127\n",
      "Epoch 214/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5224 - accuracy: 0.7090\n",
      "Epoch 215/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.5195 - accuracy: 0.7164\n",
      "Epoch 216/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.5232 - accuracy: 0.7201\n",
      "Epoch 217/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5195 - accuracy: 0.7201\n",
      "Epoch 218/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5219 - accuracy: 0.7164\n",
      "Epoch 219/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5202 - accuracy: 0.7090\n",
      "Epoch 220/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.5187 - accuracy: 0.7127\n",
      "Epoch 221/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5252 - accuracy: 0.7090\n",
      "Epoch 222/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5182 - accuracy: 0.7052\n",
      "Epoch 223/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.5198 - accuracy: 0.7090\n",
      "Epoch 224/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.5169 - accuracy: 0.7090\n",
      "Epoch 225/1500\n",
      "5/5 [==============================] - 0s 660us/step - loss: 0.5191 - accuracy: 0.7090\n",
      "Epoch 226/1500\n",
      "5/5 [==============================] - 0s 577us/step - loss: 0.5234 - accuracy: 0.7127\n",
      "Epoch 227/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5172 - accuracy: 0.7127\n",
      "Epoch 228/1500\n",
      "5/5 [==============================] - 0s 803us/step - loss: 0.5229 - accuracy: 0.7127\n",
      "Epoch 229/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5203 - accuracy: 0.7164\n",
      "Epoch 230/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.5191 - accuracy: 0.7127\n",
      "Epoch 231/1500\n",
      "5/5 [==============================] - 0s 595us/step - loss: 0.5160 - accuracy: 0.7164\n",
      "Epoch 232/1500\n",
      "5/5 [==============================] - 0s 725us/step - loss: 0.5215 - accuracy: 0.7201\n",
      "Epoch 233/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5186 - accuracy: 0.7127\n",
      "Epoch 234/1500\n",
      "5/5 [==============================] - 0s 576us/step - loss: 0.5159 - accuracy: 0.7127\n",
      "Epoch 235/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5155 - accuracy: 0.7127\n",
      "Epoch 236/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5154 - accuracy: 0.7052\n",
      "Epoch 237/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5171 - accuracy: 0.7127\n",
      "Epoch 238/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5156 - accuracy: 0.7127\n",
      "Epoch 239/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5188 - accuracy: 0.7127\n",
      "Epoch 240/1500\n",
      "5/5 [==============================] - 0s 575us/step - loss: 0.5226 - accuracy: 0.7090\n",
      "Epoch 241/1500\n",
      "5/5 [==============================] - 0s 622us/step - loss: 0.5176 - accuracy: 0.7127\n",
      "Epoch 242/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5188 - accuracy: 0.7201\n",
      "Epoch 243/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.5155 - accuracy: 0.6978\n",
      "Epoch 244/1500\n",
      "5/5 [==============================] - 0s 390us/step - loss: 0.5270 - accuracy: 0.6978\n",
      "Epoch 245/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.5283 - accuracy: 0.7052\n",
      "Epoch 246/1500\n",
      "5/5 [==============================] - 0s 619us/step - loss: 0.5296 - accuracy: 0.7127\n",
      "Epoch 247/1500\n",
      "5/5 [==============================] - 0s 411us/step - loss: 0.5169 - accuracy: 0.7052\n",
      "Epoch 248/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.5238 - accuracy: 0.6978\n",
      "Epoch 249/1500\n",
      "5/5 [==============================] - 0s 782us/step - loss: 0.5166 - accuracy: 0.7090\n",
      "Epoch 250/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.5205 - accuracy: 0.7164\n",
      "Epoch 251/1500\n",
      "5/5 [==============================] - 0s 619us/step - loss: 0.5159 - accuracy: 0.7127\n",
      "Epoch 252/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5181 - accuracy: 0.7127\n",
      "Epoch 253/1500\n",
      "5/5 [==============================] - 0s 545us/step - loss: 0.5155 - accuracy: 0.7052\n",
      "Epoch 254/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.5147 - accuracy: 0.7052\n",
      "Epoch 255/1500\n",
      "5/5 [==============================] - 0s 810us/step - loss: 0.5146 - accuracy: 0.7164\n",
      "Epoch 256/1500\n",
      "5/5 [==============================] - 0s 596us/step - loss: 0.5177 - accuracy: 0.7239\n",
      "Epoch 257/1500\n",
      "5/5 [==============================] - 0s 411us/step - loss: 0.5154 - accuracy: 0.7090\n",
      "Epoch 258/1500\n",
      "5/5 [==============================] - 0s 789us/step - loss: 0.5154 - accuracy: 0.7239\n",
      "Epoch 259/1500\n",
      "5/5 [==============================] - 0s 754us/step - loss: 0.5126 - accuracy: 0.7127\n",
      "Epoch 260/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5131 - accuracy: 0.7090\n",
      "Epoch 261/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5118 - accuracy: 0.7052\n",
      "Epoch 262/1500\n",
      "5/5 [==============================] - 0s 627us/step - loss: 0.5146 - accuracy: 0.7164\n",
      "Epoch 263/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5111 - accuracy: 0.7127\n",
      "Epoch 264/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5171 - accuracy: 0.7201\n",
      "Epoch 265/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.5113 - accuracy: 0.7127\n",
      "Epoch 266/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5166 - accuracy: 0.7090\n",
      "Epoch 267/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5097 - accuracy: 0.7164\n",
      "Epoch 268/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.5207 - accuracy: 0.7090\n",
      "Epoch 269/1500\n",
      "5/5 [==============================] - 0s 613us/step - loss: 0.5164 - accuracy: 0.7239\n",
      "Epoch 270/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5168 - accuracy: 0.7090\n",
      "Epoch 271/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.5181 - accuracy: 0.7090\n",
      "Epoch 272/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.5134 - accuracy: 0.7052\n",
      "Epoch 273/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5184 - accuracy: 0.7164\n",
      "Epoch 274/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5090 - accuracy: 0.7164\n",
      "Epoch 275/1500\n",
      "5/5 [==============================] - 0s 625us/step - loss: 0.5133 - accuracy: 0.7127\n",
      "Epoch 276/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5117 - accuracy: 0.7052\n",
      "Epoch 277/1500\n",
      "5/5 [==============================] - 0s 705us/step - loss: 0.5154 - accuracy: 0.7201\n",
      "Epoch 278/1500\n",
      "5/5 [==============================] - 0s 657us/step - loss: 0.5097 - accuracy: 0.7127\n",
      "Epoch 279/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5137 - accuracy: 0.7052\n",
      "Epoch 280/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5143 - accuracy: 0.7015\n",
      "Epoch 281/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5146 - accuracy: 0.7127\n",
      "Epoch 282/1500\n",
      "5/5 [==============================] - 0s 766us/step - loss: 0.5091 - accuracy: 0.7090\n",
      "Epoch 283/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.5093 - accuracy: 0.7090\n",
      "Epoch 284/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5076 - accuracy: 0.7164\n",
      "Epoch 285/1500\n",
      "5/5 [==============================] - 0s 579us/step - loss: 0.5130 - accuracy: 0.7276\n",
      "Epoch 286/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5091 - accuracy: 0.7239\n",
      "Epoch 287/1500\n",
      "5/5 [==============================] - 0s 595us/step - loss: 0.5120 - accuracy: 0.7090\n",
      "Epoch 288/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5097 - accuracy: 0.7090\n",
      "Epoch 289/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.5081 - accuracy: 0.7090\n",
      "Epoch 290/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5087 - accuracy: 0.7127\n",
      "Epoch 291/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.5081 - accuracy: 0.7127\n",
      "Epoch 292/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5103 - accuracy: 0.7127\n",
      "Epoch 293/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.5084 - accuracy: 0.7201\n",
      "Epoch 294/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5069 - accuracy: 0.7127\n",
      "Epoch 295/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5068 - accuracy: 0.7127\n",
      "Epoch 296/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5126 - accuracy: 0.7127\n",
      "Epoch 297/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.5145 - accuracy: 0.7052\n",
      "Epoch 298/1500\n",
      "5/5 [==============================] - 0s 584us/step - loss: 0.5101 - accuracy: 0.7164\n",
      "Epoch 299/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.5080 - accuracy: 0.7164\n",
      "Epoch 300/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5093 - accuracy: 0.7164\n",
      "Epoch 301/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.5111 - accuracy: 0.7239\n",
      "Epoch 302/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5086 - accuracy: 0.7239\n",
      "Epoch 303/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.5097 - accuracy: 0.7090\n",
      "Epoch 304/1500\n",
      "5/5 [==============================] - 0s 789us/step - loss: 0.5103 - accuracy: 0.7239\n",
      "Epoch 305/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5076 - accuracy: 0.7164\n",
      "Epoch 306/1500\n",
      "5/5 [==============================] - 0s 790us/step - loss: 0.5103 - accuracy: 0.7127\n",
      "Epoch 307/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5048 - accuracy: 0.7127\n",
      "Epoch 308/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5094 - accuracy: 0.7201\n",
      "Epoch 309/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5133 - accuracy: 0.7201\n",
      "Epoch 310/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5055 - accuracy: 0.7164\n",
      "Epoch 311/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5099 - accuracy: 0.7090\n",
      "Epoch 312/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5060 - accuracy: 0.7164\n",
      "Epoch 313/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5108 - accuracy: 0.7239\n",
      "Epoch 314/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5153 - accuracy: 0.7164\n",
      "Epoch 315/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5220 - accuracy: 0.6978\n",
      "Epoch 316/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.5280 - accuracy: 0.7127\n",
      "Epoch 317/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5431 - accuracy: 0.7052\n",
      "Epoch 318/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5087 - accuracy: 0.7090\n",
      "Epoch 319/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5270 - accuracy: 0.7127\n",
      "Epoch 320/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5021 - accuracy: 0.7313\n",
      "Epoch 321/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5309 - accuracy: 0.7127\n",
      "Epoch 322/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 598us/step - loss: 0.5050 - accuracy: 0.7239\n",
      "Epoch 323/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5136 - accuracy: 0.7052\n",
      "Epoch 324/1500\n",
      "5/5 [==============================] - 0s 614us/step - loss: 0.5123 - accuracy: 0.7015\n",
      "Epoch 325/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5071 - accuracy: 0.7239\n",
      "Epoch 326/1500\n",
      "5/5 [==============================] - 0s 613us/step - loss: 0.5075 - accuracy: 0.7090\n",
      "Epoch 327/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5134 - accuracy: 0.7090\n",
      "Epoch 328/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.5102 - accuracy: 0.7090\n",
      "Epoch 329/1500\n",
      "5/5 [==============================] - 0s 395us/step - loss: 0.5186 - accuracy: 0.7164\n",
      "Epoch 330/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5033 - accuracy: 0.7239\n",
      "Epoch 331/1500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 0.5112 - accuracy: 0.7127\n",
      "Epoch 332/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4999 - accuracy: 0.7090\n",
      "Epoch 333/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.5116 - accuracy: 0.7201\n",
      "Epoch 334/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5026 - accuracy: 0.7127\n",
      "Epoch 335/1500\n",
      "5/5 [==============================] - 0s 691us/step - loss: 0.5407 - accuracy: 0.6978\n",
      "Epoch 336/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5093 - accuracy: 0.7015\n",
      "Epoch 337/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5292 - accuracy: 0.7090\n",
      "Epoch 338/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.5039 - accuracy: 0.7201\n",
      "Epoch 339/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.5224 - accuracy: 0.6978\n",
      "Epoch 340/1500\n",
      "5/5 [==============================] - 0s 790us/step - loss: 0.5065 - accuracy: 0.7090\n",
      "Epoch 341/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.5024 - accuracy: 0.7313\n",
      "Epoch 342/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.5021 - accuracy: 0.7276\n",
      "Epoch 343/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5007 - accuracy: 0.7090\n",
      "Epoch 344/1500\n",
      "5/5 [==============================] - 0s 619us/step - loss: 0.5014 - accuracy: 0.7015\n",
      "Epoch 345/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.5026 - accuracy: 0.7201\n",
      "Epoch 346/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5015 - accuracy: 0.7201\n",
      "Epoch 347/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5041 - accuracy: 0.7090\n",
      "Epoch 348/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.5045 - accuracy: 0.7201\n",
      "Epoch 349/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.5010 - accuracy: 0.7239\n",
      "Epoch 350/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.5011 - accuracy: 0.7127\n",
      "Epoch 351/1500\n",
      "5/5 [==============================] - 0s 688us/step - loss: 0.5052 - accuracy: 0.7127\n",
      "Epoch 352/1500\n",
      "5/5 [==============================] - 0s 536us/step - loss: 0.4992 - accuracy: 0.7052\n",
      "Epoch 353/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5002 - accuracy: 0.7201\n",
      "Epoch 354/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5039 - accuracy: 0.7164\n",
      "Epoch 355/1500\n",
      "5/5 [==============================] - 0s 576us/step - loss: 0.5002 - accuracy: 0.7127\n",
      "Epoch 356/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5109 - accuracy: 0.7127\n",
      "Epoch 357/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5093 - accuracy: 0.7090\n",
      "Epoch 358/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5166 - accuracy: 0.7201\n",
      "Epoch 359/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4985 - accuracy: 0.7201\n",
      "Epoch 360/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4990 - accuracy: 0.7164\n",
      "Epoch 361/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4988 - accuracy: 0.7164\n",
      "Epoch 362/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4965 - accuracy: 0.7276\n",
      "Epoch 363/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.5081 - accuracy: 0.7201\n",
      "Epoch 364/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5026 - accuracy: 0.7239\n",
      "Epoch 365/1500\n",
      "5/5 [==============================] - 0s 659us/step - loss: 0.5047 - accuracy: 0.7201\n",
      "Epoch 366/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5051 - accuracy: 0.7239\n",
      "Epoch 367/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5003 - accuracy: 0.7313\n",
      "Epoch 368/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4993 - accuracy: 0.7164\n",
      "Epoch 369/1500\n",
      "5/5 [==============================] - 0s 523us/step - loss: 0.4985 - accuracy: 0.7201\n",
      "Epoch 370/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.4979 - accuracy: 0.7313\n",
      "Epoch 371/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4978 - accuracy: 0.7388\n",
      "Epoch 372/1500\n",
      "5/5 [==============================] - 0s 663us/step - loss: 0.4990 - accuracy: 0.7351\n",
      "Epoch 373/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4981 - accuracy: 0.7276\n",
      "Epoch 374/1500\n",
      "5/5 [==============================] - 0s 767us/step - loss: 0.4993 - accuracy: 0.7313\n",
      "Epoch 375/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5020 - accuracy: 0.7015\n",
      "Epoch 376/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5087 - accuracy: 0.7164\n",
      "Epoch 377/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5051 - accuracy: 0.7351\n",
      "Epoch 378/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5033 - accuracy: 0.7351\n",
      "Epoch 379/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4990 - accuracy: 0.7164\n",
      "Epoch 380/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4993 - accuracy: 0.7164\n",
      "Epoch 381/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4982 - accuracy: 0.7052\n",
      "Epoch 382/1500\n",
      "5/5 [==============================] - 0s 662us/step - loss: 0.4962 - accuracy: 0.7164\n",
      "Epoch 383/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4940 - accuracy: 0.7239\n",
      "Epoch 384/1500\n",
      "5/5 [==============================] - 0s 710us/step - loss: 0.4930 - accuracy: 0.7276\n",
      "Epoch 385/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4950 - accuracy: 0.7127\n",
      "Epoch 386/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4960 - accuracy: 0.7052\n",
      "Epoch 387/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5072 - accuracy: 0.7276\n",
      "Epoch 388/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5151 - accuracy: 0.7164\n",
      "Epoch 389/1500\n",
      "5/5 [==============================] - 0s 572us/step - loss: 0.4901 - accuracy: 0.7127\n",
      "Epoch 390/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5107 - accuracy: 0.7015\n",
      "Epoch 391/1500\n",
      "5/5 [==============================] - 0s 708us/step - loss: 0.5051 - accuracy: 0.7090\n",
      "Epoch 392/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4920 - accuracy: 0.7276\n",
      "Epoch 393/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4986 - accuracy: 0.7276\n",
      "Epoch 394/1500\n",
      "5/5 [==============================] - 0s 796us/step - loss: 0.4980 - accuracy: 0.7164\n",
      "Epoch 395/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4940 - accuracy: 0.7090\n",
      "Epoch 396/1500\n",
      "5/5 [==============================] - 0s 793us/step - loss: 0.4926 - accuracy: 0.7239\n",
      "Epoch 397/1500\n",
      "5/5 [==============================] - 0s 606us/step - loss: 0.4928 - accuracy: 0.7239\n",
      "Epoch 398/1500\n",
      "5/5 [==============================] - 0s 403us/step - loss: 0.4957 - accuracy: 0.7127\n",
      "Epoch 399/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4944 - accuracy: 0.7090\n",
      "Epoch 400/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4934 - accuracy: 0.7351\n",
      "Epoch 401/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4947 - accuracy: 0.7313\n",
      "Epoch 402/1500\n",
      "5/5 [==============================] - 0s 582us/step - loss: 0.4904 - accuracy: 0.7127\n",
      "Epoch 403/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4920 - accuracy: 0.7090\n",
      "Epoch 404/1500\n",
      "5/5 [==============================] - 0s 803us/step - loss: 0.4969 - accuracy: 0.7239\n",
      "Epoch 405/1500\n",
      "5/5 [==============================] - 0s 575us/step - loss: 0.5043 - accuracy: 0.7239\n",
      "Epoch 406/1500\n",
      "5/5 [==============================] - 0s 581us/step - loss: 0.4939 - accuracy: 0.7127\n",
      "Epoch 407/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.5006 - accuracy: 0.6978\n",
      "Epoch 408/1500\n",
      "5/5 [==============================] - 0s 606us/step - loss: 0.4925 - accuracy: 0.7276\n",
      "Epoch 409/1500\n",
      "5/5 [==============================] - 0s 405us/step - loss: 0.4954 - accuracy: 0.7351\n",
      "Epoch 410/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4917 - accuracy: 0.7313\n",
      "Epoch 411/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4900 - accuracy: 0.7313\n",
      "Epoch 412/1500\n",
      "5/5 [==============================] - 0s 409us/step - loss: 0.4933 - accuracy: 0.7201\n",
      "Epoch 413/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4939 - accuracy: 0.7276\n",
      "Epoch 414/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4920 - accuracy: 0.7313\n",
      "Epoch 415/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4981 - accuracy: 0.7127\n",
      "Epoch 416/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4928 - accuracy: 0.7090\n",
      "Epoch 417/1500\n",
      "5/5 [==============================] - 0s 919us/step - loss: 0.4895 - accuracy: 0.7239\n",
      "Epoch 418/1500\n",
      "5/5 [==============================] - 0s 754us/step - loss: 0.4895 - accuracy: 0.7090\n",
      "Epoch 419/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.5015 - accuracy: 0.7015\n",
      "Epoch 420/1500\n",
      "5/5 [==============================] - 0s 713us/step - loss: 0.4905 - accuracy: 0.7164\n",
      "Epoch 421/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4982 - accuracy: 0.7239\n",
      "Epoch 422/1500\n",
      "5/5 [==============================] - 0s 583us/step - loss: 0.4963 - accuracy: 0.7276\n",
      "Epoch 423/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4869 - accuracy: 0.7201\n",
      "Epoch 424/1500\n",
      "5/5 [==============================] - 0s 777us/step - loss: 0.4908 - accuracy: 0.7052\n",
      "Epoch 425/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4875 - accuracy: 0.7276\n",
      "Epoch 426/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4937 - accuracy: 0.7313\n",
      "Epoch 427/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4938 - accuracy: 0.7313\n",
      "Epoch 428/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4912 - accuracy: 0.7090\n",
      "Epoch 429/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.4925 - accuracy: 0.7164\n",
      "Epoch 430/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4929 - accuracy: 0.7164\n",
      "Epoch 431/1500\n",
      "5/5 [==============================] - 0s 583us/step - loss: 0.4908 - accuracy: 0.7239\n",
      "Epoch 432/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4997 - accuracy: 0.6978\n",
      "Epoch 433/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.5149 - accuracy: 0.7052\n",
      "Epoch 434/1500\n",
      "5/5 [==============================] - 0s 671us/step - loss: 0.4990 - accuracy: 0.7276\n",
      "Epoch 435/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4931 - accuracy: 0.7201\n",
      "Epoch 436/1500\n",
      "5/5 [==============================] - 0s 784us/step - loss: 0.4943 - accuracy: 0.7015\n",
      "Epoch 437/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4870 - accuracy: 0.7276\n",
      "Epoch 438/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.5022 - accuracy: 0.7201\n",
      "Epoch 439/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4839 - accuracy: 0.7313\n",
      "Epoch 440/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.5038 - accuracy: 0.6978\n",
      "Epoch 441/1500\n",
      "5/5 [==============================] - 0s 630us/step - loss: 0.4935 - accuracy: 0.7090\n",
      "Epoch 442/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4969 - accuracy: 0.7276\n",
      "Epoch 443/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4929 - accuracy: 0.7239\n",
      "Epoch 444/1500\n",
      "5/5 [==============================] - 0s 677us/step - loss: 0.4971 - accuracy: 0.6978\n",
      "Epoch 445/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4895 - accuracy: 0.7239\n",
      "Epoch 446/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.5034 - accuracy: 0.7201\n",
      "Epoch 447/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4974 - accuracy: 0.7388\n",
      "Epoch 448/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4905 - accuracy: 0.7127\n",
      "Epoch 449/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4892 - accuracy: 0.7276\n",
      "Epoch 450/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4883 - accuracy: 0.7313\n",
      "Epoch 451/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.5003 - accuracy: 0.7015\n",
      "Epoch 452/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4845 - accuracy: 0.7127\n",
      "Epoch 453/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4989 - accuracy: 0.7239\n",
      "Epoch 454/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4977 - accuracy: 0.7276\n",
      "Epoch 455/1500\n",
      "5/5 [==============================] - 0s 623us/step - loss: 0.4878 - accuracy: 0.7201\n",
      "Epoch 456/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4895 - accuracy: 0.7164\n",
      "Epoch 457/1500\n",
      "5/5 [==============================] - 0s 796us/step - loss: 0.4881 - accuracy: 0.7239\n",
      "Epoch 458/1500\n",
      "5/5 [==============================] - 0s 414us/step - loss: 0.4885 - accuracy: 0.7239\n",
      "Epoch 459/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.4885 - accuracy: 0.7052\n",
      "Epoch 460/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4913 - accuracy: 0.7388\n",
      "Epoch 461/1500\n",
      "5/5 [==============================] - 0s 779us/step - loss: 0.4912 - accuracy: 0.7351\n",
      "Epoch 462/1500\n",
      "5/5 [==============================] - 0s 602us/step - loss: 0.4895 - accuracy: 0.7052\n",
      "Epoch 463/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4927 - accuracy: 0.7239\n",
      "Epoch 464/1500\n",
      "5/5 [==============================] - 0s 768us/step - loss: 0.4819 - accuracy: 0.7351\n",
      "Epoch 465/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.5214 - accuracy: 0.6978\n",
      "Epoch 466/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4959 - accuracy: 0.6978\n",
      "Epoch 467/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4898 - accuracy: 0.7351\n",
      "Epoch 468/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4836 - accuracy: 0.7351\n",
      "Epoch 469/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4898 - accuracy: 0.7052\n",
      "Epoch 470/1500\n",
      "5/5 [==============================] - 0s 624us/step - loss: 0.4891 - accuracy: 0.7239\n",
      "Epoch 471/1500\n",
      "5/5 [==============================] - 0s 646us/step - loss: 0.4929 - accuracy: 0.7313\n",
      "Epoch 472/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4855 - accuracy: 0.7276\n",
      "Epoch 473/1500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 0.4905 - accuracy: 0.7052\n",
      "Epoch 474/1500\n",
      "5/5 [==============================] - 0s 653us/step - loss: 0.4824 - accuracy: 0.7388\n",
      "Epoch 475/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4856 - accuracy: 0.7313\n",
      "Epoch 476/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4838 - accuracy: 0.7127\n",
      "Epoch 477/1500\n",
      "5/5 [==============================] - 0s 681us/step - loss: 0.4838 - accuracy: 0.7127\n",
      "Epoch 478/1500\n",
      "5/5 [==============================] - 0s 626us/step - loss: 0.4906 - accuracy: 0.7164\n",
      "Epoch 479/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4838 - accuracy: 0.7164\n",
      "Epoch 480/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4860 - accuracy: 0.7127\n",
      "Epoch 481/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4850 - accuracy: 0.7127\n",
      "Epoch 482/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 637us/step - loss: 0.4839 - accuracy: 0.7313\n",
      "Epoch 483/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4852 - accuracy: 0.7090\n",
      "Epoch 484/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4851 - accuracy: 0.7201\n",
      "Epoch 485/1500\n",
      "5/5 [==============================] - 0s 411us/step - loss: 0.4889 - accuracy: 0.7127\n",
      "Epoch 486/1500\n",
      "5/5 [==============================] - 0s 834us/step - loss: 0.4844 - accuracy: 0.7351\n",
      "Epoch 487/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4868 - accuracy: 0.7388\n",
      "Epoch 488/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4835 - accuracy: 0.7164\n",
      "Epoch 489/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4910 - accuracy: 0.7090\n",
      "Epoch 490/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4841 - accuracy: 0.7276\n",
      "Epoch 491/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4836 - accuracy: 0.7313\n",
      "Epoch 492/1500\n",
      "5/5 [==============================] - 0s 799us/step - loss: 0.4832 - accuracy: 0.7313\n",
      "Epoch 493/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4822 - accuracy: 0.7351\n",
      "Epoch 494/1500\n",
      "5/5 [==============================] - 0s 665us/step - loss: 0.4843 - accuracy: 0.7239\n",
      "Epoch 495/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4827 - accuracy: 0.7425\n",
      "Epoch 496/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4886 - accuracy: 0.7276\n",
      "Epoch 497/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4853 - accuracy: 0.7276\n",
      "Epoch 498/1500\n",
      "5/5 [==============================] - 0s 582us/step - loss: 0.4926 - accuracy: 0.7201\n",
      "Epoch 499/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.4933 - accuracy: 0.7239\n",
      "Epoch 500/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4886 - accuracy: 0.7313\n",
      "Epoch 501/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4931 - accuracy: 0.7090\n",
      "Epoch 502/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4846 - accuracy: 0.7313\n",
      "Epoch 503/1500\n",
      "5/5 [==============================] - 0s 613us/step - loss: 0.4854 - accuracy: 0.7164\n",
      "Epoch 504/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4841 - accuracy: 0.7276\n",
      "Epoch 505/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4869 - accuracy: 0.7313\n",
      "Epoch 506/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4816 - accuracy: 0.7388\n",
      "Epoch 507/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.4820 - accuracy: 0.7276\n",
      "Epoch 508/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4855 - accuracy: 0.7313\n",
      "Epoch 509/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4827 - accuracy: 0.7388\n",
      "Epoch 510/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4827 - accuracy: 0.7201\n",
      "Epoch 511/1500\n",
      "5/5 [==============================] - 0s 620us/step - loss: 0.4840 - accuracy: 0.7127\n",
      "Epoch 512/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4836 - accuracy: 0.7313\n",
      "Epoch 513/1500\n",
      "5/5 [==============================] - 0s 812us/step - loss: 0.4958 - accuracy: 0.7276\n",
      "Epoch 514/1500\n",
      "5/5 [==============================] - 0s 505us/step - loss: 0.4864 - accuracy: 0.7313\n",
      "Epoch 515/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4824 - accuracy: 0.7239\n",
      "Epoch 516/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4895 - accuracy: 0.7164\n",
      "Epoch 517/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4904 - accuracy: 0.7090\n",
      "Epoch 518/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4876 - accuracy: 0.7239\n",
      "Epoch 519/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4889 - accuracy: 0.7239\n",
      "Epoch 520/1500\n",
      "5/5 [==============================] - 0s 673us/step - loss: 0.4826 - accuracy: 0.7201\n",
      "Epoch 521/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.4825 - accuracy: 0.7201\n",
      "Epoch 522/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4838 - accuracy: 0.7201\n",
      "Epoch 523/1500\n",
      "5/5 [==============================] - 0s 562us/step - loss: 0.4820 - accuracy: 0.7201\n",
      "Epoch 524/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4826 - accuracy: 0.7164\n",
      "Epoch 525/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4802 - accuracy: 0.7388\n",
      "Epoch 526/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4819 - accuracy: 0.7351\n",
      "Epoch 527/1500\n",
      "5/5 [==============================] - 0s 415us/step - loss: 0.4828 - accuracy: 0.7313\n",
      "Epoch 528/1500\n",
      "5/5 [==============================] - 0s 792us/step - loss: 0.4870 - accuracy: 0.7276\n",
      "Epoch 529/1500\n",
      "5/5 [==============================] - 0s 789us/step - loss: 0.4828 - accuracy: 0.7201\n",
      "Epoch 530/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4942 - accuracy: 0.7052\n",
      "Epoch 531/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4861 - accuracy: 0.7201\n",
      "Epoch 532/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4890 - accuracy: 0.7313\n",
      "Epoch 533/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4821 - accuracy: 0.7127\n",
      "Epoch 534/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4838 - accuracy: 0.7164\n",
      "Epoch 535/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4812 - accuracy: 0.7239\n",
      "Epoch 536/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4820 - accuracy: 0.7201\n",
      "Epoch 537/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4855 - accuracy: 0.7351\n",
      "Epoch 538/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4960 - accuracy: 0.7239\n",
      "Epoch 539/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4905 - accuracy: 0.7127\n",
      "Epoch 540/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4893 - accuracy: 0.7164\n",
      "Epoch 541/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4954 - accuracy: 0.7276\n",
      "Epoch 542/1500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 0.4952 - accuracy: 0.7351\n",
      "Epoch 543/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4845 - accuracy: 0.7127\n",
      "Epoch 544/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4845 - accuracy: 0.7127\n",
      "Epoch 545/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4798 - accuracy: 0.7276\n",
      "Epoch 546/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4813 - accuracy: 0.7351\n",
      "Epoch 547/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4786 - accuracy: 0.7351\n",
      "Epoch 548/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4792 - accuracy: 0.7388\n",
      "Epoch 549/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4807 - accuracy: 0.7276\n",
      "Epoch 550/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4830 - accuracy: 0.7276\n",
      "Epoch 551/1500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 0.4816 - accuracy: 0.7351\n",
      "Epoch 552/1500\n",
      "5/5 [==============================] - 0s 616us/step - loss: 0.4814 - accuracy: 0.7388\n",
      "Epoch 553/1500\n",
      "5/5 [==============================] - 0s 780us/step - loss: 0.4799 - accuracy: 0.7351\n",
      "Epoch 554/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4793 - accuracy: 0.7276\n",
      "Epoch 555/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4783 - accuracy: 0.7351\n",
      "Epoch 556/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4794 - accuracy: 0.7388\n",
      "Epoch 557/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4783 - accuracy: 0.7313\n",
      "Epoch 558/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4799 - accuracy: 0.7127\n",
      "Epoch 559/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4821 - accuracy: 0.7164\n",
      "Epoch 560/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4791 - accuracy: 0.7164\n",
      "Epoch 561/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4909 - accuracy: 0.7239\n",
      "Epoch 562/1500\n",
      "5/5 [==============================] - 0s 675us/step - loss: 0.4897 - accuracy: 0.7276\n",
      "Epoch 563/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4846 - accuracy: 0.7201\n",
      "Epoch 564/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4839 - accuracy: 0.7164\n",
      "Epoch 565/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4876 - accuracy: 0.7351\n",
      "Epoch 566/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4820 - accuracy: 0.7388\n",
      "Epoch 567/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4806 - accuracy: 0.7164\n",
      "Epoch 568/1500\n",
      "5/5 [==============================] - 0s 820us/step - loss: 0.4793 - accuracy: 0.7201\n",
      "Epoch 569/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4817 - accuracy: 0.7388\n",
      "Epoch 570/1500\n",
      "5/5 [==============================] - 0s 396us/step - loss: 0.4836 - accuracy: 0.7351\n",
      "Epoch 571/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4797 - accuracy: 0.7425\n",
      "Epoch 572/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4801 - accuracy: 0.7388\n",
      "Epoch 573/1500\n",
      "5/5 [==============================] - 0s 614us/step - loss: 0.4788 - accuracy: 0.7164\n",
      "Epoch 574/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4796 - accuracy: 0.7201\n",
      "Epoch 575/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4778 - accuracy: 0.7351\n",
      "Epoch 576/1500\n",
      "5/5 [==============================] - 0s 674us/step - loss: 0.4833 - accuracy: 0.7201\n",
      "Epoch 577/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.4738 - accuracy: 0.7239\n",
      "Epoch 578/1500\n",
      "5/5 [==============================] - 0s 577us/step - loss: 0.4896 - accuracy: 0.7313\n",
      "Epoch 579/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4803 - accuracy: 0.7351\n",
      "Epoch 580/1500\n",
      "5/5 [==============================] - 0s 583us/step - loss: 0.4821 - accuracy: 0.7015\n",
      "Epoch 581/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4777 - accuracy: 0.7351\n",
      "Epoch 582/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4829 - accuracy: 0.7313\n",
      "Epoch 583/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4854 - accuracy: 0.7164\n",
      "Epoch 584/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4821 - accuracy: 0.7090\n",
      "Epoch 585/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4760 - accuracy: 0.7313\n",
      "Epoch 586/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4772 - accuracy: 0.7201\n",
      "Epoch 587/1500\n",
      "5/5 [==============================] - 0s 782us/step - loss: 0.4781 - accuracy: 0.7388\n",
      "Epoch 588/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4784 - accuracy: 0.7388\n",
      "Epoch 589/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4754 - accuracy: 0.7201\n",
      "Epoch 590/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4807 - accuracy: 0.7127\n",
      "Epoch 591/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4766 - accuracy: 0.7313\n",
      "Epoch 592/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4877 - accuracy: 0.7313\n",
      "Epoch 593/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4769 - accuracy: 0.7351\n",
      "Epoch 594/1500\n",
      "5/5 [==============================] - 0s 830us/step - loss: 0.4773 - accuracy: 0.7313\n",
      "Epoch 595/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4787 - accuracy: 0.7425\n",
      "Epoch 596/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4776 - accuracy: 0.7425\n",
      "Epoch 597/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4787 - accuracy: 0.7164\n",
      "Epoch 598/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4756 - accuracy: 0.7313\n",
      "Epoch 599/1500\n",
      "5/5 [==============================] - 0s 584us/step - loss: 0.4833 - accuracy: 0.7276\n",
      "Epoch 600/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4806 - accuracy: 0.7201\n",
      "Epoch 601/1500\n",
      "5/5 [==============================] - 0s 408us/step - loss: 0.4825 - accuracy: 0.7052\n",
      "Epoch 602/1500\n",
      "5/5 [==============================] - 0s 620us/step - loss: 0.4755 - accuracy: 0.7201\n",
      "Epoch 603/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4795 - accuracy: 0.7313\n",
      "Epoch 604/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4758 - accuracy: 0.7425\n",
      "Epoch 605/1500\n",
      "5/5 [==============================] - 0s 567us/step - loss: 0.4806 - accuracy: 0.7090\n",
      "Epoch 606/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4795 - accuracy: 0.7127\n",
      "Epoch 607/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4768 - accuracy: 0.7164\n",
      "Epoch 608/1500\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7201\n",
      "Epoch 609/1500\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.4772 - accuracy: 0.7276\n",
      "Epoch 610/1500\n",
      "5/5 [==============================] - 0s 613us/step - loss: 0.4968 - accuracy: 0.7052\n",
      "Epoch 611/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4789 - accuracy: 0.7127\n",
      "Epoch 612/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4859 - accuracy: 0.7313\n",
      "Epoch 613/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4778 - accuracy: 0.7276\n",
      "Epoch 614/1500\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.67 - 0s 599us/step - loss: 0.4820 - accuracy: 0.7127\n",
      "Epoch 615/1500\n",
      "5/5 [==============================] - 0s 627us/step - loss: 0.4754 - accuracy: 0.7313\n",
      "Epoch 616/1500\n",
      "5/5 [==============================] - 0s 596us/step - loss: 0.4815 - accuracy: 0.7276\n",
      "Epoch 617/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4777 - accuracy: 0.7239\n",
      "Epoch 618/1500\n",
      "5/5 [==============================] - 0s 619us/step - loss: 0.4844 - accuracy: 0.7276\n",
      "Epoch 619/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4756 - accuracy: 0.7351\n",
      "Epoch 620/1500\n",
      "5/5 [==============================] - 0s 412us/step - loss: 0.4769 - accuracy: 0.7164\n",
      "Epoch 621/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4829 - accuracy: 0.7127\n",
      "Epoch 622/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4841 - accuracy: 0.7201\n",
      "Epoch 623/1500\n",
      "5/5 [==============================] - 0s 394us/step - loss: 0.4812 - accuracy: 0.7127\n",
      "Epoch 624/1500\n",
      "5/5 [==============================] - 0s 796us/step - loss: 0.4743 - accuracy: 0.7239\n",
      "Epoch 625/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4797 - accuracy: 0.7313\n",
      "Epoch 626/1500\n",
      "5/5 [==============================] - 0s 481us/step - loss: 0.4863 - accuracy: 0.7351\n",
      "Epoch 627/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4724 - accuracy: 0.7239\n",
      "Epoch 628/1500\n",
      "5/5 [==============================] - 0s 804us/step - loss: 0.4812 - accuracy: 0.7201\n",
      "Epoch 629/1500\n",
      "5/5 [==============================] - 0s 720us/step - loss: 0.4881 - accuracy: 0.7127\n",
      "Epoch 630/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4835 - accuracy: 0.7052\n",
      "Epoch 631/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4755 - accuracy: 0.7351\n",
      "Epoch 632/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4757 - accuracy: 0.7351\n",
      "Epoch 633/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4756 - accuracy: 0.7276\n",
      "Epoch 634/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4726 - accuracy: 0.7351\n",
      "Epoch 635/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4768 - accuracy: 0.7388\n",
      "Epoch 636/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4715 - accuracy: 0.7313\n",
      "Epoch 637/1500\n",
      "5/5 [==============================] - 0s 583us/step - loss: 0.5019 - accuracy: 0.7015\n",
      "Epoch 638/1500\n",
      "5/5 [==============================] - 0s 615us/step - loss: 0.4756 - accuracy: 0.7276\n",
      "Epoch 639/1500\n",
      "5/5 [==============================] - 0s 404us/step - loss: 0.4780 - accuracy: 0.7276\n",
      "Epoch 640/1500\n",
      "5/5 [==============================] - 0s 791us/step - loss: 0.4744 - accuracy: 0.7201\n",
      "Epoch 641/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4716 - accuracy: 0.7313\n",
      "Epoch 642/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 598us/step - loss: 0.4778 - accuracy: 0.7313\n",
      "Epoch 643/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4771 - accuracy: 0.7351\n",
      "Epoch 644/1500\n",
      "5/5 [==============================] - 0s 622us/step - loss: 0.4758 - accuracy: 0.7388\n",
      "Epoch 645/1500\n",
      "5/5 [==============================] - 0s 618us/step - loss: 0.4805 - accuracy: 0.7127\n",
      "Epoch 646/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4774 - accuracy: 0.7239\n",
      "Epoch 647/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4772 - accuracy: 0.7276\n",
      "Epoch 648/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4756 - accuracy: 0.7351\n",
      "Epoch 649/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4743 - accuracy: 0.7164\n",
      "Epoch 650/1500\n",
      "5/5 [==============================] - 0s 793us/step - loss: 0.4744 - accuracy: 0.7164\n",
      "Epoch 651/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4752 - accuracy: 0.7351\n",
      "Epoch 652/1500\n",
      "5/5 [==============================] - 0s 630us/step - loss: 0.4717 - accuracy: 0.7388\n",
      "Epoch 653/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4750 - accuracy: 0.7164\n",
      "Epoch 654/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4810 - accuracy: 0.7351\n",
      "Epoch 655/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4767 - accuracy: 0.7239\n",
      "Epoch 656/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4741 - accuracy: 0.7164\n",
      "Epoch 657/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4707 - accuracy: 0.7351\n",
      "Epoch 658/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4720 - accuracy: 0.7388\n",
      "Epoch 659/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4722 - accuracy: 0.7388\n",
      "Epoch 660/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4678 - accuracy: 0.7313\n",
      "Epoch 661/1500\n",
      "5/5 [==============================] - 0s 634us/step - loss: 0.4740 - accuracy: 0.7276\n",
      "Epoch 662/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4707 - accuracy: 0.7276\n",
      "Epoch 663/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4719 - accuracy: 0.7388\n",
      "Epoch 664/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4729 - accuracy: 0.7313\n",
      "Epoch 665/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4765 - accuracy: 0.7239\n",
      "Epoch 666/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4721 - accuracy: 0.7201\n",
      "Epoch 667/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4731 - accuracy: 0.7276\n",
      "Epoch 668/1500\n",
      "5/5 [==============================] - 0s 507us/step - loss: 0.4762 - accuracy: 0.7164\n",
      "Epoch 669/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4739 - accuracy: 0.7201\n",
      "Epoch 670/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4708 - accuracy: 0.7164\n",
      "Epoch 671/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4726 - accuracy: 0.7351\n",
      "Epoch 672/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4709 - accuracy: 0.7351\n",
      "Epoch 673/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4686 - accuracy: 0.7201\n",
      "Epoch 674/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4718 - accuracy: 0.7276\n",
      "Epoch 675/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4727 - accuracy: 0.7201\n",
      "Epoch 676/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4784 - accuracy: 0.7276\n",
      "Epoch 677/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4774 - accuracy: 0.7388\n",
      "Epoch 678/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4837 - accuracy: 0.7127\n",
      "Epoch 679/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4794 - accuracy: 0.7164\n",
      "Epoch 680/1500\n",
      "5/5 [==============================] - 0s 681us/step - loss: 0.4693 - accuracy: 0.7313\n",
      "Epoch 681/1500\n",
      "5/5 [==============================] - 0s 604us/step - loss: 0.4710 - accuracy: 0.7201\n",
      "Epoch 682/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4684 - accuracy: 0.7239\n",
      "Epoch 683/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4791 - accuracy: 0.7313\n",
      "Epoch 684/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4716 - accuracy: 0.7201\n",
      "Epoch 685/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4705 - accuracy: 0.7239\n",
      "Epoch 686/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4698 - accuracy: 0.7351\n",
      "Epoch 687/1500\n",
      "5/5 [==============================] - 0s 628us/step - loss: 0.4663 - accuracy: 0.7351\n",
      "Epoch 688/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4691 - accuracy: 0.7351\n",
      "Epoch 689/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4672 - accuracy: 0.7313\n",
      "Epoch 690/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4672 - accuracy: 0.7276\n",
      "Epoch 691/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4687 - accuracy: 0.7201\n",
      "Epoch 692/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4681 - accuracy: 0.7127\n",
      "Epoch 693/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4728 - accuracy: 0.7313\n",
      "Epoch 694/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4806 - accuracy: 0.7239\n",
      "Epoch 695/1500\n",
      "5/5 [==============================] - 0s 602us/step - loss: 0.4706 - accuracy: 0.7239\n",
      "Epoch 696/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4664 - accuracy: 0.7239\n",
      "Epoch 697/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4665 - accuracy: 0.7351\n",
      "Epoch 698/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.4749 - accuracy: 0.7276\n",
      "Epoch 699/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4661 - accuracy: 0.7388\n",
      "Epoch 700/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4732 - accuracy: 0.7127\n",
      "Epoch 701/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4749 - accuracy: 0.7015\n",
      "Epoch 702/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4649 - accuracy: 0.7388\n",
      "Epoch 703/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4648 - accuracy: 0.7388\n",
      "Epoch 704/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4661 - accuracy: 0.7201\n",
      "Epoch 705/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4671 - accuracy: 0.7164\n",
      "Epoch 706/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4651 - accuracy: 0.7239\n",
      "Epoch 707/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4701 - accuracy: 0.7164\n",
      "Epoch 708/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4699 - accuracy: 0.7276\n",
      "Epoch 709/1500\n",
      "5/5 [==============================] - 0s 810us/step - loss: 0.4823 - accuracy: 0.7127\n",
      "Epoch 710/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4746 - accuracy: 0.7388\n",
      "Epoch 711/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4782 - accuracy: 0.7351\n",
      "Epoch 712/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4677 - accuracy: 0.7313\n",
      "Epoch 713/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4676 - accuracy: 0.7201\n",
      "Epoch 714/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4636 - accuracy: 0.7351\n",
      "Epoch 715/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4654 - accuracy: 0.7201\n",
      "Epoch 716/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4672 - accuracy: 0.7201\n",
      "Epoch 717/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4701 - accuracy: 0.7276\n",
      "Epoch 718/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4668 - accuracy: 0.7239\n",
      "Epoch 719/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4634 - accuracy: 0.7388\n",
      "Epoch 720/1500\n",
      "5/5 [==============================] - 0s 554us/step - loss: 0.4654 - accuracy: 0.7239\n",
      "Epoch 721/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4675 - accuracy: 0.7276\n",
      "Epoch 722/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4625 - accuracy: 0.7351\n",
      "Epoch 723/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4721 - accuracy: 0.7090\n",
      "Epoch 724/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4646 - accuracy: 0.7239\n",
      "Epoch 725/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4684 - accuracy: 0.7164\n",
      "Epoch 726/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4662 - accuracy: 0.7239\n",
      "Epoch 727/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4799 - accuracy: 0.7351\n",
      "Epoch 728/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4629 - accuracy: 0.7276\n",
      "Epoch 729/1500\n",
      "5/5 [==============================] - 0s 619us/step - loss: 0.4670 - accuracy: 0.7090\n",
      "Epoch 730/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4600 - accuracy: 0.7351\n",
      "Epoch 731/1500\n",
      "5/5 [==============================] - 0s 615us/step - loss: 0.4648 - accuracy: 0.7425\n",
      "Epoch 732/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4621 - accuracy: 0.7388\n",
      "Epoch 733/1500\n",
      "5/5 [==============================] - 0s 583us/step - loss: 0.4646 - accuracy: 0.7351\n",
      "Epoch 734/1500\n",
      "5/5 [==============================] - 0s 581us/step - loss: 0.4682 - accuracy: 0.7276\n",
      "Epoch 735/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4641 - accuracy: 0.7388\n",
      "Epoch 736/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4623 - accuracy: 0.7201\n",
      "Epoch 737/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4592 - accuracy: 0.7201\n",
      "Epoch 738/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4621 - accuracy: 0.7351\n",
      "Epoch 739/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4681 - accuracy: 0.7276\n",
      "Epoch 740/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4654 - accuracy: 0.7313\n",
      "Epoch 741/1500\n",
      "5/5 [==============================] - 0s 421us/step - loss: 0.4663 - accuracy: 0.7276\n",
      "Epoch 742/1500\n",
      "5/5 [==============================] - 0s 613us/step - loss: 0.4625 - accuracy: 0.7201\n",
      "Epoch 743/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4648 - accuracy: 0.7313\n",
      "Epoch 744/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4610 - accuracy: 0.7351\n",
      "Epoch 745/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4635 - accuracy: 0.7201\n",
      "Epoch 746/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4674 - accuracy: 0.7276\n",
      "Epoch 747/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4616 - accuracy: 0.7388\n",
      "Epoch 748/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4592 - accuracy: 0.7351\n",
      "Epoch 749/1500\n",
      "5/5 [==============================] - 0s 789us/step - loss: 0.4582 - accuracy: 0.7276\n",
      "Epoch 750/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4585 - accuracy: 0.7239\n",
      "Epoch 751/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4590 - accuracy: 0.7201\n",
      "Epoch 752/1500\n",
      "5/5 [==============================] - 0s 579us/step - loss: 0.4581 - accuracy: 0.7276\n",
      "Epoch 753/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4584 - accuracy: 0.7388\n",
      "Epoch 754/1500\n",
      "5/5 [==============================] - 0s 506us/step - loss: 0.4585 - accuracy: 0.7313\n",
      "Epoch 755/1500\n",
      "5/5 [==============================] - 0s 606us/step - loss: 0.4615 - accuracy: 0.7239\n",
      "Epoch 756/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4639 - accuracy: 0.7239\n",
      "Epoch 757/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4616 - accuracy: 0.7351\n",
      "Epoch 758/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4626 - accuracy: 0.7201\n",
      "Epoch 759/1500\n",
      "5/5 [==============================] - 0s 794us/step - loss: 0.4677 - accuracy: 0.7313\n",
      "Epoch 760/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4810 - accuracy: 0.7201\n",
      "Epoch 761/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4735 - accuracy: 0.7127\n",
      "Epoch 762/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4721 - accuracy: 0.7164\n",
      "Epoch 763/1500\n",
      "5/5 [==============================] - 0s 812us/step - loss: 0.4693 - accuracy: 0.7313\n",
      "Epoch 764/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4655 - accuracy: 0.7313\n",
      "Epoch 765/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4661 - accuracy: 0.7313\n",
      "Epoch 766/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4647 - accuracy: 0.7239\n",
      "Epoch 767/1500\n",
      "5/5 [==============================] - 0s 614us/step - loss: 0.4669 - accuracy: 0.7351\n",
      "Epoch 768/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4673 - accuracy: 0.7239\n",
      "Epoch 769/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4758 - accuracy: 0.7164\n",
      "Epoch 770/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4788 - accuracy: 0.7201\n",
      "Epoch 771/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4684 - accuracy: 0.7239\n",
      "Epoch 772/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4682 - accuracy: 0.7276\n",
      "Epoch 773/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4746 - accuracy: 0.7239\n",
      "Epoch 774/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4706 - accuracy: 0.7351\n",
      "Epoch 775/1500\n",
      "5/5 [==============================] - 0s 604us/step - loss: 0.4669 - accuracy: 0.7201\n",
      "Epoch 776/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4556 - accuracy: 0.7388\n",
      "Epoch 777/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4702 - accuracy: 0.7201\n",
      "Epoch 778/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4558 - accuracy: 0.7313\n",
      "Epoch 779/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4758 - accuracy: 0.7090\n",
      "Epoch 780/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4611 - accuracy: 0.7127\n",
      "Epoch 781/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4551 - accuracy: 0.7276\n",
      "Epoch 782/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4605 - accuracy: 0.7201\n",
      "Epoch 783/1500\n",
      "5/5 [==============================] - 0s 782us/step - loss: 0.4577 - accuracy: 0.7351\n",
      "Epoch 784/1500\n",
      "5/5 [==============================] - 0s 606us/step - loss: 0.4562 - accuracy: 0.7351\n",
      "Epoch 785/1500\n",
      "5/5 [==============================] - 0s 869us/step - loss: 0.4580 - accuracy: 0.7313\n",
      "Epoch 786/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4582 - accuracy: 0.7276\n",
      "Epoch 787/1500\n",
      "5/5 [==============================] - 0s 602us/step - loss: 0.4567 - accuracy: 0.7351\n",
      "Epoch 788/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4590 - accuracy: 0.7276\n",
      "Epoch 789/1500\n",
      "5/5 [==============================] - 0s 411us/step - loss: 0.4564 - accuracy: 0.7351\n",
      "Epoch 790/1500\n",
      "5/5 [==============================] - 0s 788us/step - loss: 0.4610 - accuracy: 0.7313\n",
      "Epoch 791/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4596 - accuracy: 0.7276\n",
      "Epoch 792/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4646 - accuracy: 0.7313\n",
      "Epoch 793/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4536 - accuracy: 0.7351\n",
      "Epoch 794/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4814 - accuracy: 0.7015\n",
      "Epoch 795/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4698 - accuracy: 0.7052\n",
      "Epoch 796/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4588 - accuracy: 0.7313\n",
      "Epoch 797/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4610 - accuracy: 0.7351\n",
      "Epoch 798/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4647 - accuracy: 0.7052\n",
      "Epoch 799/1500\n",
      "5/5 [==============================] - 0s 797us/step - loss: 0.4609 - accuracy: 0.7313\n",
      "Epoch 800/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4604 - accuracy: 0.7388\n",
      "Epoch 801/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4622 - accuracy: 0.7164\n",
      "Epoch 802/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 786us/step - loss: 0.4659 - accuracy: 0.7201\n",
      "Epoch 803/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4620 - accuracy: 0.7276\n",
      "Epoch 804/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4571 - accuracy: 0.7351\n",
      "Epoch 805/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4536 - accuracy: 0.7239\n",
      "Epoch 806/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4581 - accuracy: 0.7276\n",
      "Epoch 807/1500\n",
      "5/5 [==============================] - 0s 622us/step - loss: 0.4576 - accuracy: 0.7351\n",
      "Epoch 808/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4565 - accuracy: 0.7164\n",
      "Epoch 809/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4563 - accuracy: 0.7313\n",
      "Epoch 810/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4538 - accuracy: 0.7351\n",
      "Epoch 811/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4568 - accuracy: 0.7351\n",
      "Epoch 812/1500\n",
      "5/5 [==============================] - 0s 807us/step - loss: 0.4539 - accuracy: 0.7239\n",
      "Epoch 813/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4644 - accuracy: 0.7164\n",
      "Epoch 814/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4599 - accuracy: 0.7313\n",
      "Epoch 815/1500\n",
      "5/5 [==============================] - 0s 803us/step - loss: 0.4641 - accuracy: 0.7239\n",
      "Epoch 816/1500\n",
      "5/5 [==============================] - 0s 596us/step - loss: 0.4715 - accuracy: 0.7015\n",
      "Epoch 817/1500\n",
      "5/5 [==============================] - 0s 386us/step - loss: 0.4949 - accuracy: 0.7015\n",
      "Epoch 818/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4637 - accuracy: 0.7276\n",
      "Epoch 819/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4555 - accuracy: 0.7239\n",
      "Epoch 820/1500\n",
      "5/5 [==============================] - 0s 595us/step - loss: 0.4600 - accuracy: 0.7127\n",
      "Epoch 821/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4564 - accuracy: 0.7313\n",
      "Epoch 822/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4600 - accuracy: 0.7127\n",
      "Epoch 823/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4553 - accuracy: 0.7276\n",
      "Epoch 824/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.4600 - accuracy: 0.7276\n",
      "Epoch 825/1500\n",
      "5/5 [==============================] - 0s 604us/step - loss: 0.4518 - accuracy: 0.7239\n",
      "Epoch 826/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4570 - accuracy: 0.7388\n",
      "Epoch 827/1500\n",
      "5/5 [==============================] - 0s 421us/step - loss: 0.4551 - accuracy: 0.7388\n",
      "Epoch 828/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4541 - accuracy: 0.7276\n",
      "Epoch 829/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4513 - accuracy: 0.7351\n",
      "Epoch 830/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4552 - accuracy: 0.7276\n",
      "Epoch 831/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4485 - accuracy: 0.7239\n",
      "Epoch 832/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4530 - accuracy: 0.7276\n",
      "Epoch 833/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4512 - accuracy: 0.7276\n",
      "Epoch 834/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4492 - accuracy: 0.7313\n",
      "Epoch 835/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4492 - accuracy: 0.7313\n",
      "Epoch 836/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4527 - accuracy: 0.7351\n",
      "Epoch 837/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4536 - accuracy: 0.7351\n",
      "Epoch 838/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4536 - accuracy: 0.7276\n",
      "Epoch 839/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4612 - accuracy: 0.7239\n",
      "Epoch 840/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4580 - accuracy: 0.7127\n",
      "Epoch 841/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4541 - accuracy: 0.7201\n",
      "Epoch 842/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4516 - accuracy: 0.7239\n",
      "Epoch 843/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4500 - accuracy: 0.7276\n",
      "Epoch 844/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4509 - accuracy: 0.7239\n",
      "Epoch 845/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4531 - accuracy: 0.7313\n",
      "Epoch 846/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4619 - accuracy: 0.7276\n",
      "Epoch 847/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4569 - accuracy: 0.7201\n",
      "Epoch 848/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4521 - accuracy: 0.7201\n",
      "Epoch 849/1500\n",
      "5/5 [==============================] - 0s 789us/step - loss: 0.4683 - accuracy: 0.7201\n",
      "Epoch 850/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4477 - accuracy: 0.7313\n",
      "Epoch 851/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4608 - accuracy: 0.7052\n",
      "Epoch 852/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4491 - accuracy: 0.7313\n",
      "Epoch 853/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4494 - accuracy: 0.7313\n",
      "Epoch 854/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4530 - accuracy: 0.7239\n",
      "Epoch 855/1500\n",
      "5/5 [==============================] - 0s 617us/step - loss: 0.4519 - accuracy: 0.7313\n",
      "Epoch 856/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4490 - accuracy: 0.7276\n",
      "Epoch 857/1500\n",
      "5/5 [==============================] - 0s 557us/step - loss: 0.4800 - accuracy: 0.7015\n",
      "Epoch 858/1500\n",
      "5/5 [==============================] - 0s 606us/step - loss: 0.4738 - accuracy: 0.7052\n",
      "Epoch 859/1500\n",
      "5/5 [==============================] - 0s 604us/step - loss: 0.4786 - accuracy: 0.7127\n",
      "Epoch 860/1500\n",
      "5/5 [==============================] - 0s 525us/step - loss: 0.4589 - accuracy: 0.7127\n",
      "Epoch 861/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4613 - accuracy: 0.7239\n",
      "Epoch 862/1500\n",
      "5/5 [==============================] - 0s 582us/step - loss: 0.4558 - accuracy: 0.7388\n",
      "Epoch 863/1500\n",
      "5/5 [==============================] - 0s 397us/step - loss: 0.4520 - accuracy: 0.7276\n",
      "Epoch 864/1500\n",
      "5/5 [==============================] - 0s 576us/step - loss: 0.4512 - accuracy: 0.7201\n",
      "Epoch 865/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4483 - accuracy: 0.7239\n",
      "Epoch 866/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4581 - accuracy: 0.7388\n",
      "Epoch 867/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4571 - accuracy: 0.7239\n",
      "Epoch 868/1500\n",
      "5/5 [==============================] - 0s 789us/step - loss: 0.4756 - accuracy: 0.7090\n",
      "Epoch 869/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4557 - accuracy: 0.7201\n",
      "Epoch 870/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4522 - accuracy: 0.7351\n",
      "Epoch 871/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4523 - accuracy: 0.7201\n",
      "Epoch 872/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4509 - accuracy: 0.7164\n",
      "Epoch 873/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4506 - accuracy: 0.7239\n",
      "Epoch 874/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4505 - accuracy: 0.7351\n",
      "Epoch 875/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4534 - accuracy: 0.7276\n",
      "Epoch 876/1500\n",
      "5/5 [==============================] - 0s 818us/step - loss: 0.4468 - accuracy: 0.7276\n",
      "Epoch 877/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4638 - accuracy: 0.7164\n",
      "Epoch 878/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4512 - accuracy: 0.7239\n",
      "Epoch 879/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4592 - accuracy: 0.7201\n",
      "Epoch 880/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4475 - accuracy: 0.7425\n",
      "Epoch 881/1500\n",
      "5/5 [==============================] - 0s 595us/step - loss: 0.4805 - accuracy: 0.7127\n",
      "Epoch 882/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4506 - accuracy: 0.7351\n",
      "Epoch 883/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4625 - accuracy: 0.7388\n",
      "Epoch 884/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4735 - accuracy: 0.7164\n",
      "Epoch 885/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4629 - accuracy: 0.7239\n",
      "Epoch 886/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4657 - accuracy: 0.7313\n",
      "Epoch 887/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4560 - accuracy: 0.7351\n",
      "Epoch 888/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4522 - accuracy: 0.7351\n",
      "Epoch 889/1500\n",
      "5/5 [==============================] - 0s 595us/step - loss: 0.4507 - accuracy: 0.7351\n",
      "Epoch 890/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4537 - accuracy: 0.7276\n",
      "Epoch 891/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4562 - accuracy: 0.7388\n",
      "Epoch 892/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4530 - accuracy: 0.7313\n",
      "Epoch 893/1500\n",
      "5/5 [==============================] - 0s 445us/step - loss: 0.4540 - accuracy: 0.7276\n",
      "Epoch 894/1500\n",
      "5/5 [==============================] - 0s 582us/step - loss: 0.4608 - accuracy: 0.7164\n",
      "Epoch 895/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4508 - accuracy: 0.7239\n",
      "Epoch 896/1500\n",
      "5/5 [==============================] - 0s 421us/step - loss: 0.4624 - accuracy: 0.7239\n",
      "Epoch 897/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4557 - accuracy: 0.7351\n",
      "Epoch 898/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4495 - accuracy: 0.7239\n",
      "Epoch 899/1500\n",
      "5/5 [==============================] - 0s 583us/step - loss: 0.4476 - accuracy: 0.7351\n",
      "Epoch 900/1500\n",
      "5/5 [==============================] - 0s 408us/step - loss: 0.4474 - accuracy: 0.7388\n",
      "Epoch 901/1500\n",
      "5/5 [==============================] - 0s 788us/step - loss: 0.4487 - accuracy: 0.7276\n",
      "Epoch 902/1500\n",
      "5/5 [==============================] - 0s 625us/step - loss: 0.4492 - accuracy: 0.7239\n",
      "Epoch 903/1500\n",
      "5/5 [==============================] - 0s 739us/step - loss: 0.4437 - accuracy: 0.7388\n",
      "Epoch 904/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4483 - accuracy: 0.7388\n",
      "Epoch 905/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4470 - accuracy: 0.7351\n",
      "Epoch 906/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4503 - accuracy: 0.7351\n",
      "Epoch 907/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4459 - accuracy: 0.7276\n",
      "Epoch 908/1500\n",
      "5/5 [==============================] - 0s 602us/step - loss: 0.4548 - accuracy: 0.7201\n",
      "Epoch 909/1500\n",
      "5/5 [==============================] - 0s 507us/step - loss: 0.4487 - accuracy: 0.7313\n",
      "Epoch 910/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4575 - accuracy: 0.7313\n",
      "Epoch 911/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4506 - accuracy: 0.7388\n",
      "Epoch 912/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4502 - accuracy: 0.7201\n",
      "Epoch 913/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4448 - accuracy: 0.7276\n",
      "Epoch 914/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4481 - accuracy: 0.7351\n",
      "Epoch 915/1500\n",
      "5/5 [==============================] - 0s 782us/step - loss: 0.4463 - accuracy: 0.7388\n",
      "Epoch 916/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4445 - accuracy: 0.7388\n",
      "Epoch 917/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4457 - accuracy: 0.7313\n",
      "Epoch 918/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4460 - accuracy: 0.7313\n",
      "Epoch 919/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4461 - accuracy: 0.7313\n",
      "Epoch 920/1500\n",
      "5/5 [==============================] - 0s 624us/step - loss: 0.4470 - accuracy: 0.7239\n",
      "Epoch 921/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4487 - accuracy: 0.7239\n",
      "Epoch 922/1500\n",
      "5/5 [==============================] - 0s 688us/step - loss: 0.4477 - accuracy: 0.7276\n",
      "Epoch 923/1500\n",
      "5/5 [==============================] - 0s 555us/step - loss: 0.4443 - accuracy: 0.7239\n",
      "Epoch 924/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4455 - accuracy: 0.7351\n",
      "Epoch 925/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4434 - accuracy: 0.7276\n",
      "Epoch 926/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4451 - accuracy: 0.7239\n",
      "Epoch 927/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4451 - accuracy: 0.7351\n",
      "Epoch 928/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4427 - accuracy: 0.7276\n",
      "Epoch 929/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4596 - accuracy: 0.7164\n",
      "Epoch 930/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4403 - accuracy: 0.7239\n",
      "Epoch 931/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.4651 - accuracy: 0.7276\n",
      "Epoch 932/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4555 - accuracy: 0.7313\n",
      "Epoch 933/1500\n",
      "5/5 [==============================] - 0s 584us/step - loss: 0.4680 - accuracy: 0.7090\n",
      "Epoch 934/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4575 - accuracy: 0.7164\n",
      "Epoch 935/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4489 - accuracy: 0.7276\n",
      "Epoch 936/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4485 - accuracy: 0.7201\n",
      "Epoch 937/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4474 - accuracy: 0.7276\n",
      "Epoch 938/1500\n",
      "5/5 [==============================] - 0s 584us/step - loss: 0.4684 - accuracy: 0.7351\n",
      "Epoch 939/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4554 - accuracy: 0.7239\n",
      "Epoch 940/1500\n",
      "5/5 [==============================] - 0s 411us/step - loss: 0.4610 - accuracy: 0.7201\n",
      "Epoch 941/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.4505 - accuracy: 0.7276\n",
      "Epoch 942/1500\n",
      "5/5 [==============================] - 0s 705us/step - loss: 0.4487 - accuracy: 0.7351\n",
      "Epoch 943/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4439 - accuracy: 0.7388\n",
      "Epoch 944/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4478 - accuracy: 0.7201\n",
      "Epoch 945/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4488 - accuracy: 0.7388\n",
      "Epoch 946/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4457 - accuracy: 0.7313\n",
      "Epoch 947/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4550 - accuracy: 0.7164\n",
      "Epoch 948/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4427 - accuracy: 0.7276\n",
      "Epoch 949/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4449 - accuracy: 0.7388\n",
      "Epoch 950/1500\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.71 - 0s 598us/step - loss: 0.4448 - accuracy: 0.7351\n",
      "Epoch 951/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4415 - accuracy: 0.7388\n",
      "Epoch 952/1500\n",
      "5/5 [==============================] - 0s 584us/step - loss: 0.4451 - accuracy: 0.7351\n",
      "Epoch 953/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4426 - accuracy: 0.7276\n",
      "Epoch 954/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4432 - accuracy: 0.7201\n",
      "Epoch 955/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4432 - accuracy: 0.7276\n",
      "Epoch 956/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4419 - accuracy: 0.7201\n",
      "Epoch 957/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4484 - accuracy: 0.7276\n",
      "Epoch 958/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4424 - accuracy: 0.7276\n",
      "Epoch 959/1500\n",
      "5/5 [==============================] - 0s 670us/step - loss: 0.4431 - accuracy: 0.7239\n",
      "Epoch 960/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4428 - accuracy: 0.7201\n",
      "Epoch 961/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4427 - accuracy: 0.7164\n",
      "Epoch 962/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 597us/step - loss: 0.4493 - accuracy: 0.7201\n",
      "Epoch 963/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4403 - accuracy: 0.7313\n",
      "Epoch 964/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4459 - accuracy: 0.7313\n",
      "Epoch 965/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4476 - accuracy: 0.7201\n",
      "Epoch 966/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4403 - accuracy: 0.7388\n",
      "Epoch 967/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4415 - accuracy: 0.7612\n",
      "Epoch 968/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4442 - accuracy: 0.7761\n",
      "Epoch 969/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4449 - accuracy: 0.7910\n",
      "Epoch 970/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4430 - accuracy: 0.7985\n",
      "Epoch 971/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4418 - accuracy: 0.7799\n",
      "Epoch 972/1500\n",
      "5/5 [==============================] - 0s 627us/step - loss: 0.4383 - accuracy: 0.7649\n",
      "Epoch 973/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4432 - accuracy: 0.7649\n",
      "Epoch 974/1500\n",
      "5/5 [==============================] - 0s 618us/step - loss: 0.4425 - accuracy: 0.7649\n",
      "Epoch 975/1500\n",
      "5/5 [==============================] - 0s 666us/step - loss: 0.4436 - accuracy: 0.7687\n",
      "Epoch 976/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4382 - accuracy: 0.7724\n",
      "Epoch 977/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4389 - accuracy: 0.7910\n",
      "Epoch 978/1500\n",
      "5/5 [==============================] - 0s 596us/step - loss: 0.4415 - accuracy: 0.7910\n",
      "Epoch 979/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4419 - accuracy: 0.7836\n",
      "Epoch 980/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4474 - accuracy: 0.7799\n",
      "Epoch 981/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4431 - accuracy: 0.7724\n",
      "Epoch 982/1500\n",
      "5/5 [==============================] - 0s 664us/step - loss: 0.4438 - accuracy: 0.7761\n",
      "Epoch 983/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4409 - accuracy: 0.7910\n",
      "Epoch 984/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4434 - accuracy: 0.7724\n",
      "Epoch 985/1500\n",
      "5/5 [==============================] - 0s 645us/step - loss: 0.4433 - accuracy: 0.7948\n",
      "Epoch 986/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4410 - accuracy: 0.7836\n",
      "Epoch 987/1500\n",
      "5/5 [==============================] - 0s 582us/step - loss: 0.4395 - accuracy: 0.7649\n",
      "Epoch 988/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4416 - accuracy: 0.7612\n",
      "Epoch 989/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4477 - accuracy: 0.7799\n",
      "Epoch 990/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4458 - accuracy: 0.7836\n",
      "Epoch 991/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4427 - accuracy: 0.7836\n",
      "Epoch 992/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4434 - accuracy: 0.7836\n",
      "Epoch 993/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4386 - accuracy: 0.7910\n",
      "Epoch 994/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4391 - accuracy: 0.7836\n",
      "Epoch 995/1500\n",
      "5/5 [==============================] - 0s 556us/step - loss: 0.4488 - accuracy: 0.7649\n",
      "Epoch 996/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4446 - accuracy: 0.7687\n",
      "Epoch 997/1500\n",
      "5/5 [==============================] - 0s 412us/step - loss: 0.4418 - accuracy: 0.7724\n",
      "Epoch 998/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.4406 - accuracy: 0.7761\n",
      "Epoch 999/1500\n",
      "5/5 [==============================] - 0s 557us/step - loss: 0.4444 - accuracy: 0.7873\n",
      "Epoch 1000/1500\n",
      "5/5 [==============================] - 0s 615us/step - loss: 0.4400 - accuracy: 0.7948\n",
      "Epoch 1001/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4406 - accuracy: 0.7873\n",
      "Epoch 1002/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4377 - accuracy: 0.7873\n",
      "Epoch 1003/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4407 - accuracy: 0.7649\n",
      "Epoch 1004/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4408 - accuracy: 0.7649\n",
      "Epoch 1005/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4390 - accuracy: 0.7761\n",
      "Epoch 1006/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4411 - accuracy: 0.7761\n",
      "Epoch 1007/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4384 - accuracy: 0.7761\n",
      "Epoch 1008/1500\n",
      "5/5 [==============================] - 0s 618us/step - loss: 0.4489 - accuracy: 0.7724\n",
      "Epoch 1009/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4508 - accuracy: 0.7836\n",
      "Epoch 1010/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4619 - accuracy: 0.7724\n",
      "Epoch 1011/1500\n",
      "5/5 [==============================] - 0s 526us/step - loss: 0.4413 - accuracy: 0.8022\n",
      "Epoch 1012/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4415 - accuracy: 0.7948\n",
      "Epoch 1013/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4374 - accuracy: 0.7761\n",
      "Epoch 1014/1500\n",
      "5/5 [==============================] - 0s 602us/step - loss: 0.4438 - accuracy: 0.7799\n",
      "Epoch 1015/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4430 - accuracy: 0.7799\n",
      "Epoch 1016/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4405 - accuracy: 0.7799\n",
      "Epoch 1017/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4378 - accuracy: 0.7799\n",
      "Epoch 1018/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4345 - accuracy: 0.7761\n",
      "Epoch 1019/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4360 - accuracy: 0.7910\n",
      "Epoch 1020/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4389 - accuracy: 0.7948\n",
      "Epoch 1021/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4373 - accuracy: 0.7948\n",
      "Epoch 1022/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.4378 - accuracy: 0.7910\n",
      "Epoch 1023/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.4359 - accuracy: 0.7836\n",
      "Epoch 1024/1500\n",
      "5/5 [==============================] - 0s 781us/step - loss: 0.4368 - accuracy: 0.7724\n",
      "Epoch 1025/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4398 - accuracy: 0.7612\n",
      "Epoch 1026/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4368 - accuracy: 0.7761\n",
      "Epoch 1027/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4406 - accuracy: 0.7537\n",
      "Epoch 1028/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4406 - accuracy: 0.7761\n",
      "Epoch 1029/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4346 - accuracy: 0.7761\n",
      "Epoch 1030/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4393 - accuracy: 0.7687\n",
      "Epoch 1031/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4387 - accuracy: 0.7799\n",
      "Epoch 1032/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4560 - accuracy: 0.7836\n",
      "Epoch 1033/1500\n",
      "5/5 [==============================] - 0s 578us/step - loss: 0.4409 - accuracy: 0.7799\n",
      "Epoch 1034/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4432 - accuracy: 0.7799\n",
      "Epoch 1035/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4341 - accuracy: 0.7948\n",
      "Epoch 1036/1500\n",
      "5/5 [==============================] - 0s 572us/step - loss: 0.4382 - accuracy: 0.7910\n",
      "Epoch 1037/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4375 - accuracy: 0.7985\n",
      "Epoch 1038/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4358 - accuracy: 0.7948\n",
      "Epoch 1039/1500\n",
      "5/5 [==============================] - 0s 384us/step - loss: 0.4390 - accuracy: 0.7948\n",
      "Epoch 1040/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4404 - accuracy: 0.7873\n",
      "Epoch 1041/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4398 - accuracy: 0.7799\n",
      "Epoch 1042/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 585us/step - loss: 0.4377 - accuracy: 0.7985\n",
      "Epoch 1043/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4381 - accuracy: 0.7985\n",
      "Epoch 1044/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4367 - accuracy: 0.7985\n",
      "Epoch 1045/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4372 - accuracy: 0.7799\n",
      "Epoch 1046/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4351 - accuracy: 0.7612\n",
      "Epoch 1047/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.4380 - accuracy: 0.7836\n",
      "Epoch 1048/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4334 - accuracy: 0.7836\n",
      "Epoch 1049/1500\n",
      "5/5 [==============================] - 0s 653us/step - loss: 0.4352 - accuracy: 0.7948\n",
      "Epoch 1050/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4382 - accuracy: 0.7948\n",
      "Epoch 1051/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4398 - accuracy: 0.7836\n",
      "Epoch 1052/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4388 - accuracy: 0.7799\n",
      "Epoch 1053/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4347 - accuracy: 0.7910\n",
      "Epoch 1054/1500\n",
      "5/5 [==============================] - 0s 594us/step - loss: 0.4369 - accuracy: 0.7948\n",
      "Epoch 1055/1500\n",
      "5/5 [==============================] - 0s 689us/step - loss: 0.4349 - accuracy: 0.7948\n",
      "Epoch 1056/1500\n",
      "5/5 [==============================] - 0s 481us/step - loss: 0.4531 - accuracy: 0.7724\n",
      "Epoch 1057/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4457 - accuracy: 0.7873\n",
      "Epoch 1058/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4478 - accuracy: 0.7873\n",
      "Epoch 1059/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4371 - accuracy: 0.7985\n",
      "Epoch 1060/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4366 - accuracy: 0.7873\n",
      "Epoch 1061/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.4449 - accuracy: 0.7910\n",
      "Epoch 1062/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4498 - accuracy: 0.7910\n",
      "Epoch 1063/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4458 - accuracy: 0.7910\n",
      "Epoch 1064/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.4628 - accuracy: 0.7836\n",
      "Epoch 1065/1500\n",
      "5/5 [==============================] - 0s 624us/step - loss: 0.4429 - accuracy: 0.7873\n",
      "Epoch 1066/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4458 - accuracy: 0.7836\n",
      "Epoch 1067/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4399 - accuracy: 0.7910\n",
      "Epoch 1068/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4366 - accuracy: 0.7873\n",
      "Epoch 1069/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4396 - accuracy: 0.7836\n",
      "Epoch 1070/1500\n",
      "5/5 [==============================] - 0s 621us/step - loss: 0.4381 - accuracy: 0.7948\n",
      "Epoch 1071/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4343 - accuracy: 0.7948\n",
      "Epoch 1072/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4375 - accuracy: 0.7836\n",
      "Epoch 1073/1500\n",
      "5/5 [==============================] - 0s 794us/step - loss: 0.4361 - accuracy: 0.7985\n",
      "Epoch 1074/1500\n",
      "5/5 [==============================] - 0s 682us/step - loss: 0.4342 - accuracy: 0.7873\n",
      "Epoch 1075/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4348 - accuracy: 0.7836\n",
      "Epoch 1076/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4415 - accuracy: 0.8022\n",
      "Epoch 1077/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4328 - accuracy: 0.8022\n",
      "Epoch 1078/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4446 - accuracy: 0.7836\n",
      "Epoch 1079/1500\n",
      "5/5 [==============================] - 0s 577us/step - loss: 0.4366 - accuracy: 0.7985\n",
      "Epoch 1080/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4368 - accuracy: 0.7985\n",
      "Epoch 1081/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.4308 - accuracy: 0.8022\n",
      "Epoch 1082/1500\n",
      "5/5 [==============================] - 0s 584us/step - loss: 0.4350 - accuracy: 0.7985\n",
      "Epoch 1083/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4353 - accuracy: 0.7873\n",
      "Epoch 1084/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4326 - accuracy: 0.7836\n",
      "Epoch 1085/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4324 - accuracy: 0.7761\n",
      "Epoch 1086/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4259 - accuracy: 0.7836\n",
      "Epoch 1087/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4409 - accuracy: 0.7910\n",
      "Epoch 1088/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4301 - accuracy: 0.7948\n",
      "Epoch 1089/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4411 - accuracy: 0.7687\n",
      "Epoch 1090/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4341 - accuracy: 0.7873\n",
      "Epoch 1091/1500\n",
      "5/5 [==============================] - 0s 602us/step - loss: 0.4408 - accuracy: 0.7873\n",
      "Epoch 1092/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4399 - accuracy: 0.7910\n",
      "Epoch 1093/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4547 - accuracy: 0.7761\n",
      "Epoch 1094/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4469 - accuracy: 0.7761\n",
      "Epoch 1095/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4450 - accuracy: 0.7873\n",
      "Epoch 1096/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4359 - accuracy: 0.7910\n",
      "Epoch 1097/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4311 - accuracy: 0.7799\n",
      "Epoch 1098/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4306 - accuracy: 0.7985\n",
      "Epoch 1099/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4286 - accuracy: 0.7910\n",
      "Epoch 1100/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4362 - accuracy: 0.7799\n",
      "Epoch 1101/1500\n",
      "5/5 [==============================] - 0s 425us/step - loss: 0.4313 - accuracy: 0.7799\n",
      "Epoch 1102/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4298 - accuracy: 0.7948\n",
      "Epoch 1103/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4290 - accuracy: 0.7873\n",
      "Epoch 1104/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4286 - accuracy: 0.7910\n",
      "Epoch 1105/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4390 - accuracy: 0.7836\n",
      "Epoch 1106/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4375 - accuracy: 0.7948\n",
      "Epoch 1107/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4360 - accuracy: 0.7948\n",
      "Epoch 1108/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4362 - accuracy: 0.7873\n",
      "Epoch 1109/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4331 - accuracy: 0.7948\n",
      "Epoch 1110/1500\n",
      "5/5 [==============================] - 0s 792us/step - loss: 0.4322 - accuracy: 0.7985\n",
      "Epoch 1111/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.4341 - accuracy: 0.7910\n",
      "Epoch 1112/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4294 - accuracy: 0.7948\n",
      "Epoch 1113/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4289 - accuracy: 0.7910\n",
      "Epoch 1114/1500\n",
      "5/5 [==============================] - 0s 622us/step - loss: 0.4356 - accuracy: 0.7799\n",
      "Epoch 1115/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4365 - accuracy: 0.7761\n",
      "Epoch 1116/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4295 - accuracy: 0.7799\n",
      "Epoch 1117/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4252 - accuracy: 0.7948\n",
      "Epoch 1118/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4337 - accuracy: 0.7910\n",
      "Epoch 1119/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4307 - accuracy: 0.7910\n",
      "Epoch 1120/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4289 - accuracy: 0.7873\n",
      "Epoch 1121/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4390 - accuracy: 0.7873\n",
      "Epoch 1122/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4358 - accuracy: 0.7948\n",
      "Epoch 1123/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4304 - accuracy: 0.7985\n",
      "Epoch 1124/1500\n",
      "5/5 [==============================] - 0s 596us/step - loss: 0.4299 - accuracy: 0.7985\n",
      "Epoch 1125/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4275 - accuracy: 0.7985\n",
      "Epoch 1126/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4298 - accuracy: 0.7948\n",
      "Epoch 1127/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4283 - accuracy: 0.7985\n",
      "Epoch 1128/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4373 - accuracy: 0.7985\n",
      "Epoch 1129/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4346 - accuracy: 0.8060\n",
      "Epoch 1130/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4346 - accuracy: 0.7873\n",
      "Epoch 1131/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4324 - accuracy: 0.7910\n",
      "Epoch 1132/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4311 - accuracy: 0.7873\n",
      "Epoch 1133/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4306 - accuracy: 0.7948\n",
      "Epoch 1134/1500\n",
      "5/5 [==============================] - 0s 385us/step - loss: 0.4280 - accuracy: 0.7910\n",
      "Epoch 1135/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4278 - accuracy: 0.7910\n",
      "Epoch 1136/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4364 - accuracy: 0.7910\n",
      "Epoch 1137/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4268 - accuracy: 0.8022\n",
      "Epoch 1138/1500\n",
      "5/5 [==============================] - 0s 392us/step - loss: 0.4284 - accuracy: 0.7948\n",
      "Epoch 1139/1500\n",
      "5/5 [==============================] - 0s 796us/step - loss: 0.4338 - accuracy: 0.7910\n",
      "Epoch 1140/1500\n",
      "5/5 [==============================] - 0s 410us/step - loss: 0.4283 - accuracy: 0.7948\n",
      "Epoch 1141/1500\n",
      "5/5 [==============================] - 0s 604us/step - loss: 0.4272 - accuracy: 0.7836\n",
      "Epoch 1142/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4263 - accuracy: 0.7910\n",
      "Epoch 1143/1500\n",
      "5/5 [==============================] - 0s 793us/step - loss: 0.4279 - accuracy: 0.8022\n",
      "Epoch 1144/1500\n",
      "5/5 [==============================] - 0s 410us/step - loss: 0.4268 - accuracy: 0.8022\n",
      "Epoch 1145/1500\n",
      "5/5 [==============================] - 0s 787us/step - loss: 0.4263 - accuracy: 0.8022\n",
      "Epoch 1146/1500\n",
      "5/5 [==============================] - 0s 790us/step - loss: 0.4308 - accuracy: 0.8022\n",
      "Epoch 1147/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4292 - accuracy: 0.7910\n",
      "Epoch 1148/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4294 - accuracy: 0.7836\n",
      "Epoch 1149/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4313 - accuracy: 0.7985\n",
      "Epoch 1150/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4282 - accuracy: 0.8022\n",
      "Epoch 1151/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4270 - accuracy: 0.7799\n",
      "Epoch 1152/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4339 - accuracy: 0.7948\n",
      "Epoch 1153/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4287 - accuracy: 0.7836\n",
      "Epoch 1154/1500\n",
      "5/5 [==============================] - 0s 596us/step - loss: 0.4382 - accuracy: 0.7687\n",
      "Epoch 1155/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4249 - accuracy: 0.7985\n",
      "Epoch 1156/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4319 - accuracy: 0.7948\n",
      "Epoch 1157/1500\n",
      "5/5 [==============================] - 0s 396us/step - loss: 0.4246 - accuracy: 0.7910\n",
      "Epoch 1158/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4262 - accuracy: 0.7948\n",
      "Epoch 1159/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.4265 - accuracy: 0.8022\n",
      "Epoch 1160/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4349 - accuracy: 0.8060\n",
      "Epoch 1161/1500\n",
      "5/5 [==============================] - 0s 581us/step - loss: 0.4426 - accuracy: 0.7948\n",
      "Epoch 1162/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4396 - accuracy: 0.7761\n",
      "Epoch 1163/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4252 - accuracy: 0.8060\n",
      "Epoch 1164/1500\n",
      "5/5 [==============================] - 0s 465us/step - loss: 0.4360 - accuracy: 0.7724\n",
      "Epoch 1165/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.4264 - accuracy: 0.8022\n",
      "Epoch 1166/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4350 - accuracy: 0.7948\n",
      "Epoch 1167/1500\n",
      "5/5 [==============================] - 0s 706us/step - loss: 0.4251 - accuracy: 0.8060\n",
      "Epoch 1168/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4373 - accuracy: 0.7873\n",
      "Epoch 1169/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4334 - accuracy: 0.7873\n",
      "Epoch 1170/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4285 - accuracy: 0.7948\n",
      "Epoch 1171/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4288 - accuracy: 0.7948\n",
      "Epoch 1172/1500\n",
      "5/5 [==============================] - 0s 584us/step - loss: 0.4273 - accuracy: 0.7836\n",
      "Epoch 1173/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4234 - accuracy: 0.7910\n",
      "Epoch 1174/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4256 - accuracy: 0.7948\n",
      "Epoch 1175/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4236 - accuracy: 0.8022\n",
      "Epoch 1176/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4281 - accuracy: 0.7873\n",
      "Epoch 1177/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4277 - accuracy: 0.7836\n",
      "Epoch 1178/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4262 - accuracy: 0.7985\n",
      "Epoch 1179/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4239 - accuracy: 0.8022\n",
      "Epoch 1180/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4236 - accuracy: 0.7985\n",
      "Epoch 1181/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4290 - accuracy: 0.7799\n",
      "Epoch 1182/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4298 - accuracy: 0.8022\n",
      "Epoch 1183/1500\n",
      "5/5 [==============================] - 0s 737us/step - loss: 0.4337 - accuracy: 0.7836\n",
      "Epoch 1184/1500\n",
      "5/5 [==============================] - 0s 616us/step - loss: 0.4293 - accuracy: 0.7985\n",
      "Epoch 1185/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4237 - accuracy: 0.7948\n",
      "Epoch 1186/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4337 - accuracy: 0.8022\n",
      "Epoch 1187/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4348 - accuracy: 0.7948\n",
      "Epoch 1188/1500\n",
      "5/5 [==============================] - 0s 396us/step - loss: 0.4317 - accuracy: 0.7873\n",
      "Epoch 1189/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4262 - accuracy: 0.8022\n",
      "Epoch 1190/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4262 - accuracy: 0.7948\n",
      "Epoch 1191/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4300 - accuracy: 0.7985\n",
      "Epoch 1192/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4240 - accuracy: 0.8022\n",
      "Epoch 1193/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4386 - accuracy: 0.7649\n",
      "Epoch 1194/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4597 - accuracy: 0.7761\n",
      "Epoch 1195/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4709 - accuracy: 0.7687\n",
      "Epoch 1196/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4523 - accuracy: 0.7575\n",
      "Epoch 1197/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4560 - accuracy: 0.7687\n",
      "Epoch 1198/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4779 - accuracy: 0.7836\n",
      "Epoch 1199/1500\n",
      "5/5 [==============================] - 0s 409us/step - loss: 0.4371 - accuracy: 0.7910\n",
      "Epoch 1200/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 594us/step - loss: 0.4411 - accuracy: 0.7724\n",
      "Epoch 1201/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4381 - accuracy: 0.7649\n",
      "Epoch 1202/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4344 - accuracy: 0.7836\n",
      "Epoch 1203/1500\n",
      "5/5 [==============================] - 0s 801us/step - loss: 0.4375 - accuracy: 0.7724\n",
      "Epoch 1204/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4368 - accuracy: 0.7873\n",
      "Epoch 1205/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4304 - accuracy: 0.7761\n",
      "Epoch 1206/1500\n",
      "5/5 [==============================] - 0s 614us/step - loss: 0.4291 - accuracy: 0.7910\n",
      "Epoch 1207/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4252 - accuracy: 0.7910\n",
      "Epoch 1208/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4339 - accuracy: 0.7836\n",
      "Epoch 1209/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4253 - accuracy: 0.7836\n",
      "Epoch 1210/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4281 - accuracy: 0.7985\n",
      "Epoch 1211/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4262 - accuracy: 0.7799\n",
      "Epoch 1212/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4279 - accuracy: 0.7873\n",
      "Epoch 1213/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4246 - accuracy: 0.7836\n",
      "Epoch 1214/1500\n",
      "5/5 [==============================] - 0s 706us/step - loss: 0.4245 - accuracy: 0.7873\n",
      "Epoch 1215/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4250 - accuracy: 0.7761\n",
      "Epoch 1216/1500\n",
      "5/5 [==============================] - 0s 623us/step - loss: 0.4342 - accuracy: 0.7985\n",
      "Epoch 1217/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4231 - accuracy: 0.8134\n",
      "Epoch 1218/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4315 - accuracy: 0.7948\n",
      "Epoch 1219/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4278 - accuracy: 0.7873\n",
      "Epoch 1220/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4292 - accuracy: 0.7873\n",
      "Epoch 1221/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4307 - accuracy: 0.7910\n",
      "Epoch 1222/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4300 - accuracy: 0.7836\n",
      "Epoch 1223/1500\n",
      "5/5 [==============================] - 0s 615us/step - loss: 0.4276 - accuracy: 0.7873\n",
      "Epoch 1224/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4255 - accuracy: 0.7910\n",
      "Epoch 1225/1500\n",
      "5/5 [==============================] - 0s 422us/step - loss: 0.4283 - accuracy: 0.7873\n",
      "Epoch 1226/1500\n",
      "5/5 [==============================] - 0s 621us/step - loss: 0.4388 - accuracy: 0.7799\n",
      "Epoch 1227/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4297 - accuracy: 0.7836\n",
      "Epoch 1228/1500\n",
      "5/5 [==============================] - 0s 493us/step - loss: 0.4249 - accuracy: 0.7836\n",
      "Epoch 1229/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4247 - accuracy: 0.8022\n",
      "Epoch 1230/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4285 - accuracy: 0.8022\n",
      "Epoch 1231/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4237 - accuracy: 0.8060\n",
      "Epoch 1232/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4224 - accuracy: 0.7910\n",
      "Epoch 1233/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4232 - accuracy: 0.7948\n",
      "Epoch 1234/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4258 - accuracy: 0.7985\n",
      "Epoch 1235/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4338 - accuracy: 0.7948\n",
      "Epoch 1236/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4281 - accuracy: 0.7948\n",
      "Epoch 1237/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4241 - accuracy: 0.8022\n",
      "Epoch 1238/1500\n",
      "5/5 [==============================] - 0s 710us/step - loss: 0.4309 - accuracy: 0.7948\n",
      "Epoch 1239/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4275 - accuracy: 0.8022\n",
      "Epoch 1240/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4372 - accuracy: 0.7799\n",
      "Epoch 1241/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.4468 - accuracy: 0.7910\n",
      "Epoch 1242/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4365 - accuracy: 0.7873\n",
      "Epoch 1243/1500\n",
      "5/5 [==============================] - 0s 624us/step - loss: 0.4329 - accuracy: 0.7873\n",
      "Epoch 1244/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4303 - accuracy: 0.7836\n",
      "Epoch 1245/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4230 - accuracy: 0.7948\n",
      "Epoch 1246/1500\n",
      "5/5 [==============================] - 0s 793us/step - loss: 0.4220 - accuracy: 0.7910\n",
      "Epoch 1247/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4215 - accuracy: 0.7948\n",
      "Epoch 1248/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4254 - accuracy: 0.7873\n",
      "Epoch 1249/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4297 - accuracy: 0.7873\n",
      "Epoch 1250/1500\n",
      "5/5 [==============================] - 0s 583us/step - loss: 0.4225 - accuracy: 0.7948\n",
      "Epoch 1251/1500\n",
      "5/5 [==============================] - 0s 584us/step - loss: 0.4219 - accuracy: 0.8022\n",
      "Epoch 1252/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4242 - accuracy: 0.7985\n",
      "Epoch 1253/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4238 - accuracy: 0.7948\n",
      "Epoch 1254/1500\n",
      "5/5 [==============================] - 0s 604us/step - loss: 0.4219 - accuracy: 0.8060\n",
      "Epoch 1255/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4242 - accuracy: 0.8022\n",
      "Epoch 1256/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4308 - accuracy: 0.7910\n",
      "Epoch 1257/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4226 - accuracy: 0.7948\n",
      "Epoch 1258/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4285 - accuracy: 0.7948\n",
      "Epoch 1259/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4350 - accuracy: 0.7948\n",
      "Epoch 1260/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4214 - accuracy: 0.7948\n",
      "Epoch 1261/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4313 - accuracy: 0.7985\n",
      "Epoch 1262/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4286 - accuracy: 0.7799\n",
      "Epoch 1263/1500\n",
      "5/5 [==============================] - 0s 781us/step - loss: 0.4434 - accuracy: 0.7836\n",
      "Epoch 1264/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4486 - accuracy: 0.7687\n",
      "Epoch 1265/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4477 - accuracy: 0.7724\n",
      "Epoch 1266/1500\n",
      "5/5 [==============================] - 0s 622us/step - loss: 0.4534 - accuracy: 0.7687\n",
      "Epoch 1267/1500\n",
      "5/5 [==============================] - 0s 595us/step - loss: 0.4487 - accuracy: 0.7799\n",
      "Epoch 1268/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4443 - accuracy: 0.7799\n",
      "Epoch 1269/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4549 - accuracy: 0.7575\n",
      "Epoch 1270/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4585 - accuracy: 0.7761\n",
      "Epoch 1271/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4391 - accuracy: 0.7948\n",
      "Epoch 1272/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4338 - accuracy: 0.7910\n",
      "Epoch 1273/1500\n",
      "5/5 [==============================] - 0s 581us/step - loss: 0.4450 - accuracy: 0.7612\n",
      "Epoch 1274/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4508 - accuracy: 0.7687\n",
      "Epoch 1275/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4343 - accuracy: 0.8134\n",
      "Epoch 1276/1500\n",
      "5/5 [==============================] - 0s 799us/step - loss: 0.4597 - accuracy: 0.7873\n",
      "Epoch 1277/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4700 - accuracy: 0.7649\n",
      "Epoch 1278/1500\n",
      "5/5 [==============================] - 0s 613us/step - loss: 0.4335 - accuracy: 0.8022\n",
      "Epoch 1279/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4930 - accuracy: 0.7687\n",
      "Epoch 1280/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4478 - accuracy: 0.7799\n",
      "Epoch 1281/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4632 - accuracy: 0.7649\n",
      "Epoch 1282/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4269 - accuracy: 0.7985\n",
      "Epoch 1283/1500\n",
      "5/5 [==============================] - 0s 799us/step - loss: 0.4403 - accuracy: 0.7948\n",
      "Epoch 1284/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4366 - accuracy: 0.7836\n",
      "Epoch 1285/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4225 - accuracy: 0.7948\n",
      "Epoch 1286/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4407 - accuracy: 0.7910\n",
      "Epoch 1287/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4351 - accuracy: 0.7910\n",
      "Epoch 1288/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4293 - accuracy: 0.7948\n",
      "Epoch 1289/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4288 - accuracy: 0.7948\n",
      "Epoch 1290/1500\n",
      "5/5 [==============================] - 0s 571us/step - loss: 0.4302 - accuracy: 0.7910\n",
      "Epoch 1291/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4216 - accuracy: 0.8134\n",
      "Epoch 1292/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4799 - accuracy: 0.7799\n",
      "Epoch 1293/1500\n",
      "5/5 [==============================] - 0s 582us/step - loss: 0.4392 - accuracy: 0.7687\n",
      "Epoch 1294/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4310 - accuracy: 0.7836\n",
      "Epoch 1295/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4358 - accuracy: 0.8172\n",
      "Epoch 1296/1500\n",
      "5/5 [==============================] - 0s 544us/step - loss: 0.4321 - accuracy: 0.8134\n",
      "Epoch 1297/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4225 - accuracy: 0.7948\n",
      "Epoch 1298/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4251 - accuracy: 0.8022\n",
      "Epoch 1299/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4274 - accuracy: 0.7836\n",
      "Epoch 1300/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4272 - accuracy: 0.7948\n",
      "Epoch 1301/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4341 - accuracy: 0.8022\n",
      "Epoch 1302/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4238 - accuracy: 0.7985\n",
      "Epoch 1303/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4206 - accuracy: 0.7910\n",
      "Epoch 1304/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4259 - accuracy: 0.7948\n",
      "Epoch 1305/1500\n",
      "5/5 [==============================] - 0s 623us/step - loss: 0.4253 - accuracy: 0.8022\n",
      "Epoch 1306/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4249 - accuracy: 0.7985\n",
      "Epoch 1307/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4232 - accuracy: 0.7948\n",
      "Epoch 1308/1500\n",
      "5/5 [==============================] - 0s 811us/step - loss: 0.4218 - accuracy: 0.7948\n",
      "Epoch 1309/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4175 - accuracy: 0.8022\n",
      "Epoch 1310/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4215 - accuracy: 0.7948\n",
      "Epoch 1311/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4205 - accuracy: 0.7799\n",
      "Epoch 1312/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4194 - accuracy: 0.7985\n",
      "Epoch 1313/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4178 - accuracy: 0.8060\n",
      "Epoch 1314/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4198 - accuracy: 0.7910\n",
      "Epoch 1315/1500\n",
      "5/5 [==============================] - 0s 676us/step - loss: 0.4224 - accuracy: 0.7836\n",
      "Epoch 1316/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4222 - accuracy: 0.7910\n",
      "Epoch 1317/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4230 - accuracy: 0.7836\n",
      "Epoch 1318/1500\n",
      "5/5 [==============================] - 0s 805us/step - loss: 0.4277 - accuracy: 0.7910\n",
      "Epoch 1319/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4293 - accuracy: 0.7948\n",
      "Epoch 1320/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4258 - accuracy: 0.7910\n",
      "Epoch 1321/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4193 - accuracy: 0.7985\n",
      "Epoch 1322/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4210 - accuracy: 0.7948\n",
      "Epoch 1323/1500\n",
      "5/5 [==============================] - 0s 473us/step - loss: 0.4178 - accuracy: 0.7985\n",
      "Epoch 1324/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4207 - accuracy: 0.7985\n",
      "Epoch 1325/1500\n",
      "5/5 [==============================] - 0s 795us/step - loss: 0.4374 - accuracy: 0.7761\n",
      "Epoch 1326/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4171 - accuracy: 0.8097\n",
      "Epoch 1327/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4420 - accuracy: 0.7873\n",
      "Epoch 1328/1500\n",
      "5/5 [==============================] - 0s 591us/step - loss: 0.4196 - accuracy: 0.7985\n",
      "Epoch 1329/1500\n",
      "5/5 [==============================] - 0s 596us/step - loss: 0.4142 - accuracy: 0.7985\n",
      "Epoch 1330/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4278 - accuracy: 0.8097\n",
      "Epoch 1331/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4271 - accuracy: 0.7948\n",
      "Epoch 1332/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4205 - accuracy: 0.7985\n",
      "Epoch 1333/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4346 - accuracy: 0.7799\n",
      "Epoch 1334/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4204 - accuracy: 0.7873\n",
      "Epoch 1335/1500\n",
      "5/5 [==============================] - 0s 656us/step - loss: 0.4207 - accuracy: 0.7948\n",
      "Epoch 1336/1500\n",
      "5/5 [==============================] - 0s 415us/step - loss: 0.4198 - accuracy: 0.7873\n",
      "Epoch 1337/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4201 - accuracy: 0.7724\n",
      "Epoch 1338/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4173 - accuracy: 0.7985\n",
      "Epoch 1339/1500\n",
      "5/5 [==============================] - 0s 581us/step - loss: 0.4133 - accuracy: 0.8097\n",
      "Epoch 1340/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4219 - accuracy: 0.8209\n",
      "Epoch 1341/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4165 - accuracy: 0.8172\n",
      "Epoch 1342/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4176 - accuracy: 0.7948\n",
      "Epoch 1343/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4161 - accuracy: 0.8022\n",
      "Epoch 1344/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4178 - accuracy: 0.7948\n",
      "Epoch 1345/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4307 - accuracy: 0.7836\n",
      "Epoch 1346/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4172 - accuracy: 0.8134\n",
      "Epoch 1347/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4427 - accuracy: 0.7985\n",
      "Epoch 1348/1500\n",
      "5/5 [==============================] - 0s 552us/step - loss: 0.4138 - accuracy: 0.8097\n",
      "Epoch 1349/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4432 - accuracy: 0.7761\n",
      "Epoch 1350/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4231 - accuracy: 0.7948\n",
      "Epoch 1351/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4207 - accuracy: 0.8172\n",
      "Epoch 1352/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4199 - accuracy: 0.8097\n",
      "Epoch 1353/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4174 - accuracy: 0.8097\n",
      "Epoch 1354/1500\n",
      "5/5 [==============================] - 0s 778us/step - loss: 0.4264 - accuracy: 0.8134\n",
      "Epoch 1355/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4207 - accuracy: 0.8134\n",
      "Epoch 1356/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4215 - accuracy: 0.8060\n",
      "Epoch 1357/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4304 - accuracy: 0.7873\n",
      "Epoch 1358/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 798us/step - loss: 0.4301 - accuracy: 0.7873\n",
      "Epoch 1359/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4206 - accuracy: 0.7873\n",
      "Epoch 1360/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4172 - accuracy: 0.7985\n",
      "Epoch 1361/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4188 - accuracy: 0.7910\n",
      "Epoch 1362/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4308 - accuracy: 0.7873\n",
      "Epoch 1363/1500\n",
      "5/5 [==============================] - 0s 803us/step - loss: 0.4226 - accuracy: 0.8022\n",
      "Epoch 1364/1500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 0.4222 - accuracy: 0.8097\n",
      "Epoch 1365/1500\n",
      "5/5 [==============================] - 0s 614us/step - loss: 0.4218 - accuracy: 0.7985\n",
      "Epoch 1366/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4204 - accuracy: 0.8097\n",
      "Epoch 1367/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4381 - accuracy: 0.8134\n",
      "Epoch 1368/1500\n",
      "5/5 [==============================] - 0s 414us/step - loss: 0.4297 - accuracy: 0.8134\n",
      "Epoch 1369/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4370 - accuracy: 0.8097\n",
      "Epoch 1370/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4328 - accuracy: 0.8097\n",
      "Epoch 1371/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4294 - accuracy: 0.8134\n",
      "Epoch 1372/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4199 - accuracy: 0.8060\n",
      "Epoch 1373/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4481 - accuracy: 0.7761\n",
      "Epoch 1374/1500\n",
      "5/5 [==============================] - 0s 650us/step - loss: 0.4441 - accuracy: 0.7985\n",
      "Epoch 1375/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4350 - accuracy: 0.7873\n",
      "Epoch 1376/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4324 - accuracy: 0.7799\n",
      "Epoch 1377/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4281 - accuracy: 0.7873\n",
      "Epoch 1378/1500\n",
      "5/5 [==============================] - 0s 610us/step - loss: 0.4161 - accuracy: 0.8134\n",
      "Epoch 1379/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4214 - accuracy: 0.7985\n",
      "Epoch 1380/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4173 - accuracy: 0.8022\n",
      "Epoch 1381/1500\n",
      "5/5 [==============================] - 0s 407us/step - loss: 0.4195 - accuracy: 0.7948\n",
      "Epoch 1382/1500\n",
      "5/5 [==============================] - 0s 597us/step - loss: 0.4172 - accuracy: 0.7948\n",
      "Epoch 1383/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4138 - accuracy: 0.8134\n",
      "Epoch 1384/1500\n",
      "5/5 [==============================] - 0s 590us/step - loss: 0.4124 - accuracy: 0.8097\n",
      "Epoch 1385/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4109 - accuracy: 0.8060\n",
      "Epoch 1386/1500\n",
      "5/5 [==============================] - 0s 605us/step - loss: 0.4122 - accuracy: 0.8246\n",
      "Epoch 1387/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4158 - accuracy: 0.8284\n",
      "Epoch 1388/1500\n",
      "5/5 [==============================] - 0s 412us/step - loss: 0.4118 - accuracy: 0.8060\n",
      "Epoch 1389/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4127 - accuracy: 0.8022\n",
      "Epoch 1390/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.4094 - accuracy: 0.8060\n",
      "Epoch 1391/1500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 0.4123 - accuracy: 0.8060\n",
      "Epoch 1392/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4145 - accuracy: 0.7985\n",
      "Epoch 1393/1500\n",
      "5/5 [==============================] - 0s 793us/step - loss: 0.4134 - accuracy: 0.8097\n",
      "Epoch 1394/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4106 - accuracy: 0.8172\n",
      "Epoch 1395/1500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 0.4087 - accuracy: 0.8284\n",
      "Epoch 1396/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4110 - accuracy: 0.8134\n",
      "Epoch 1397/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4129 - accuracy: 0.8060\n",
      "Epoch 1398/1500\n",
      "5/5 [==============================] - 0s 808us/step - loss: 0.4114 - accuracy: 0.8134\n",
      "Epoch 1399/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4121 - accuracy: 0.8209\n",
      "Epoch 1400/1500\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.84 - 0s 606us/step - loss: 0.4113 - accuracy: 0.8097\n",
      "Epoch 1401/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4090 - accuracy: 0.8097\n",
      "Epoch 1402/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4093 - accuracy: 0.8134\n",
      "Epoch 1403/1500\n",
      "5/5 [==============================] - 0s 621us/step - loss: 0.4097 - accuracy: 0.8134\n",
      "Epoch 1404/1500\n",
      "5/5 [==============================] - 0s 588us/step - loss: 0.4128 - accuracy: 0.8022\n",
      "Epoch 1405/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4087 - accuracy: 0.7873\n",
      "Epoch 1406/1500\n",
      "5/5 [==============================] - 0s 620us/step - loss: 0.4133 - accuracy: 0.8134\n",
      "Epoch 1407/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4086 - accuracy: 0.8134\n",
      "Epoch 1408/1500\n",
      "5/5 [==============================] - 0s 779us/step - loss: 0.4201 - accuracy: 0.7948\n",
      "Epoch 1409/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4169 - accuracy: 0.8097\n",
      "Epoch 1410/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4351 - accuracy: 0.7948\n",
      "Epoch 1411/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4103 - accuracy: 0.8097\n",
      "Epoch 1412/1500\n",
      "5/5 [==============================] - 0s 613us/step - loss: 0.4091 - accuracy: 0.8097\n",
      "Epoch 1413/1500\n",
      "5/5 [==============================] - 0s 567us/step - loss: 0.4128 - accuracy: 0.7985\n",
      "Epoch 1414/1500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 0.4093 - accuracy: 0.8172\n",
      "Epoch 1415/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4120 - accuracy: 0.8097\n",
      "Epoch 1416/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4073 - accuracy: 0.8134\n",
      "Epoch 1417/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4177 - accuracy: 0.7948\n",
      "Epoch 1418/1500\n",
      "5/5 [==============================] - 0s 609us/step - loss: 0.4193 - accuracy: 0.8097\n",
      "Epoch 1419/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4144 - accuracy: 0.8097\n",
      "Epoch 1420/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4103 - accuracy: 0.8134\n",
      "Epoch 1421/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4205 - accuracy: 0.7985\n",
      "Epoch 1422/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4218 - accuracy: 0.7985\n",
      "Epoch 1423/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4356 - accuracy: 0.7836\n",
      "Epoch 1424/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4467 - accuracy: 0.7724\n",
      "Epoch 1425/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4387 - accuracy: 0.7724\n",
      "Epoch 1426/1500\n",
      "5/5 [==============================] - 0s 632us/step - loss: 0.4352 - accuracy: 0.7761\n",
      "Epoch 1427/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4343 - accuracy: 0.7761\n",
      "Epoch 1428/1500\n",
      "5/5 [==============================] - 0s 784us/step - loss: 0.4318 - accuracy: 0.7948\n",
      "Epoch 1429/1500\n",
      "5/5 [==============================] - 0s 608us/step - loss: 0.4318 - accuracy: 0.7948\n",
      "Epoch 1430/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4246 - accuracy: 0.7948\n",
      "Epoch 1431/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4156 - accuracy: 0.8022\n",
      "Epoch 1432/1500\n",
      "5/5 [==============================] - 0s 672us/step - loss: 0.4134 - accuracy: 0.8172\n",
      "Epoch 1433/1500\n",
      "5/5 [==============================] - 0s 471us/step - loss: 0.4164 - accuracy: 0.7985\n",
      "Epoch 1434/1500\n",
      "5/5 [==============================] - 0s 783us/step - loss: 0.4180 - accuracy: 0.8097\n",
      "Epoch 1435/1500\n",
      "5/5 [==============================] - 0s 771us/step - loss: 0.4188 - accuracy: 0.7985\n",
      "Epoch 1436/1500\n",
      "5/5 [==============================] - 0s 390us/step - loss: 0.4234 - accuracy: 0.7873\n",
      "Epoch 1437/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4155 - accuracy: 0.8060\n",
      "Epoch 1438/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4167 - accuracy: 0.8060\n",
      "Epoch 1439/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4175 - accuracy: 0.7985\n",
      "Epoch 1440/1500\n",
      "5/5 [==============================] - 0s 667us/step - loss: 0.4088 - accuracy: 0.8060\n",
      "Epoch 1441/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4128 - accuracy: 0.8134\n",
      "Epoch 1442/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4099 - accuracy: 0.8172\n",
      "Epoch 1443/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4061 - accuracy: 0.8134\n",
      "Epoch 1444/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4098 - accuracy: 0.8134\n",
      "Epoch 1445/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4080 - accuracy: 0.8097\n",
      "Epoch 1446/1500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 0.4076 - accuracy: 0.7910\n",
      "Epoch 1447/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4066 - accuracy: 0.8060\n",
      "Epoch 1448/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4079 - accuracy: 0.8022\n",
      "Epoch 1449/1500\n",
      "5/5 [==============================] - 0s 709us/step - loss: 0.4073 - accuracy: 0.8246\n",
      "Epoch 1450/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4055 - accuracy: 0.8246\n",
      "Epoch 1451/1500\n",
      "5/5 [==============================] - 0s 589us/step - loss: 0.4053 - accuracy: 0.8172\n",
      "Epoch 1452/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4051 - accuracy: 0.8172\n",
      "Epoch 1453/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4109 - accuracy: 0.8060\n",
      "Epoch 1454/1500\n",
      "5/5 [==============================] - 0s 790us/step - loss: 0.4099 - accuracy: 0.8134\n",
      "Epoch 1455/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4082 - accuracy: 0.8209\n",
      "Epoch 1456/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.4053 - accuracy: 0.8321\n",
      "Epoch 1457/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4139 - accuracy: 0.8097\n",
      "Epoch 1458/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4101 - accuracy: 0.8209\n",
      "Epoch 1459/1500\n",
      "5/5 [==============================] - 0s 655us/step - loss: 0.4205 - accuracy: 0.8022\n",
      "Epoch 1460/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4100 - accuracy: 0.8060\n",
      "Epoch 1461/1500\n",
      "5/5 [==============================] - 0s 636us/step - loss: 0.4129 - accuracy: 0.7948\n",
      "Epoch 1462/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4093 - accuracy: 0.8134\n",
      "Epoch 1463/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4048 - accuracy: 0.8209\n",
      "Epoch 1464/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4096 - accuracy: 0.8246\n",
      "Epoch 1465/1500\n",
      "5/5 [==============================] - 0s 612us/step - loss: 0.4060 - accuracy: 0.8172\n",
      "Epoch 1466/1500\n",
      "5/5 [==============================] - 0s 411us/step - loss: 0.4105 - accuracy: 0.8060\n",
      "Epoch 1467/1500\n",
      "5/5 [==============================] - 0s 797us/step - loss: 0.4129 - accuracy: 0.7910\n",
      "Epoch 1468/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4140 - accuracy: 0.7985\n",
      "Epoch 1469/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4112 - accuracy: 0.8246\n",
      "Epoch 1470/1500\n",
      "5/5 [==============================] - 0s 615us/step - loss: 0.4216 - accuracy: 0.8097\n",
      "Epoch 1471/1500\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8134\n",
      "Epoch 1472/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4093 - accuracy: 0.7985\n",
      "Epoch 1473/1500\n",
      "5/5 [==============================] - 0s 811us/step - loss: 0.4111 - accuracy: 0.8172\n",
      "Epoch 1474/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4036 - accuracy: 0.8209\n",
      "Epoch 1475/1500\n",
      "5/5 [==============================] - 0s 786us/step - loss: 0.4098 - accuracy: 0.8097\n",
      "Epoch 1476/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4067 - accuracy: 0.8246\n",
      "Epoch 1477/1500\n",
      "5/5 [==============================] - 0s 725us/step - loss: 0.4062 - accuracy: 0.8022\n",
      "Epoch 1478/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4047 - accuracy: 0.8060\n",
      "Epoch 1479/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4040 - accuracy: 0.8134\n",
      "Epoch 1480/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4052 - accuracy: 0.8060\n",
      "Epoch 1481/1500\n",
      "5/5 [==============================] - 0s 581us/step - loss: 0.4057 - accuracy: 0.8172\n",
      "Epoch 1482/1500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.4054 - accuracy: 0.8172\n",
      "Epoch 1483/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4036 - accuracy: 0.8172\n",
      "Epoch 1484/1500\n",
      "5/5 [==============================] - 0s 592us/step - loss: 0.4079 - accuracy: 0.7985\n",
      "Epoch 1485/1500\n",
      "5/5 [==============================] - 0s 798us/step - loss: 0.4041 - accuracy: 0.8022\n",
      "Epoch 1486/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4048 - accuracy: 0.8134\n",
      "Epoch 1487/1500\n",
      "5/5 [==============================] - 0s 785us/step - loss: 0.4069 - accuracy: 0.8209\n",
      "Epoch 1488/1500\n",
      "5/5 [==============================] - 0s 611us/step - loss: 0.4034 - accuracy: 0.8246\n",
      "Epoch 1489/1500\n",
      "5/5 [==============================] - 0s 766us/step - loss: 0.4099 - accuracy: 0.8209\n",
      "Epoch 1490/1500\n",
      "5/5 [==============================] - 0s 587us/step - loss: 0.4074 - accuracy: 0.8209\n",
      "Epoch 1491/1500\n",
      "5/5 [==============================] - 0s 586us/step - loss: 0.4045 - accuracy: 0.8172\n",
      "Epoch 1492/1500\n",
      "5/5 [==============================] - 0s 626us/step - loss: 0.4104 - accuracy: 0.8172\n",
      "Epoch 1493/1500\n",
      "5/5 [==============================] - 0s 766us/step - loss: 0.4115 - accuracy: 0.7985\n",
      "Epoch 1494/1500\n",
      "5/5 [==============================] - 0s 585us/step - loss: 0.4090 - accuracy: 0.8060\n",
      "Epoch 1495/1500\n",
      "5/5 [==============================] - 0s 580us/step - loss: 0.4063 - accuracy: 0.8209\n",
      "Epoch 1496/1500\n",
      "5/5 [==============================] - 0s 817us/step - loss: 0.4023 - accuracy: 0.8172\n",
      "Epoch 1497/1500\n",
      "5/5 [==============================] - 0s 548us/step - loss: 0.4126 - accuracy: 0.8134\n",
      "Epoch 1498/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4088 - accuracy: 0.8246\n",
      "Epoch 1499/1500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 0.4057 - accuracy: 0.8209\n",
      "Epoch 1500/1500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 0.4082 - accuracy: 0.8134\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6122 - accuracy: 0.7500\n",
      "accuracy:75.00%\n"
     ]
    }
   ],
   "source": [
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "df = df.sample(frac = 0.5)\n",
    "\n",
    "dataset = df.values\n",
    "X = dataset[:,0:8].astype('float')\n",
    "Y = dataset[:,8]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = 1500, batch_size = 64)\n",
    "\n",
    "scores  = model.evaluate(X_test, Y_test)\n",
    "print(\"%s:%.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
